%!TEX root = ../thesis.tex
%*******************************************************************************
%*********************************** First Chapter *****************************
%*******************************************************************************

\chapter{Introduction}  %Title of the First Chapter

\ifpdf
    \graphicspath{{Chapter1/Figs/Raster/}{Chapter1/Figs/PDF/}{Chapter1/Figs/}}
\else
    \graphicspath{{Chapter1/Figs/Vector/}{Chapter1/Figs/}}
\fi


All life on our planet is connected through a shared history recorded in its DNA.
Over time, the genomes of organisms are copied, sometimes with error or recombination.
These mutations give rise to genetic, and ultimately phenotypic diversity.
Through isolation and drift, genetic diversity enables and defines the generation of new species.

Although easily stated, this core dogma of genomics is often forgotten at the level of the most common analysis patterns used in the field.
When considering the genomes of many individuals, we frequently pluck a single related genome from the tree of life to use as a reference.
Using alignment, we express our sequencing data from the collection of samples in terms of positions and edits to the reference sequence.
We then use variant calling and phasing algorithms to filter and structure these edits into a reconstruction of the underlying haplotypes in the set of non-reference samples which we have sequenced.
We can then proceed to use the inferred genotypes and haplotypes to answer biological questions of interest.

In this way, we have not fully sequenced the new genomes, but \emph{resequenced} them against the reference genome.
Pieces of the new genomes which could not be mapped to the reference will be left out of our analysis.
Resequencing may thus distort our perspective of the genomes which we are attempting to reconstruct.
It may yield an uncertain and incomplete perspective on the total genomic complexity of the system we are assaying.

Resequencing has arisen in response to the technical properties of the most commonly-used DNA sequencing technologies.
These ``second generation'' sequencing by synthesis technologies produce abundant and inexpensive short reads of up to 250 base pairs, and in the past decade have become the largest source of data in the DNA sequencing market.

Higher sequencing costs previously motivated the application of expensive computational approaches to analyze all the sequences of interest simultaneously.
The decades prior to the development of cheap sequencing saw the use of multiple sequence alignment algorithms with high computational complexity.
Analyzing hundreds or thousands of sequences with such techniques is expensive but justifiable given the costs of acquiring them.

However, such approaches became completely inconceivable as new sequencing technologies allowed the generation of tens and then hundreds of gigabytes of data in a single run.
The new, low-cost techniques allowed joint analyses of thousands of genomes from a single species.
Resequencing provided a practical means to complete these analyses.
The alignment phase could be completed out of core, with each sample compared to the common reference genome, and only in a final phase of analysis might all the genome data be collected together for the inference of alleles at a given genetic locus.
By enabling the analysis of genomes an a previously unthinkable scale, resequencing became the core genome inference pattern in genomics.
The standardization of data formats promulgated in large genome sequencing projects supported the separation of different phases of analysis, yielding a rich ecosystem of interacting tools.

In resequencing, the reference sequence shapes the observable space in a process that is often called \emph{reference bias}.
DNA sequencing reads that contain sequence which is divergent from or not present in the reference sequence are likely to be unmapped or mismapped.
This results in lower coverage for non-reference alleles, in effect forcing new samples to appear more similar to the reference than they actually are.
Divergence itself frustrates the genome inference process, as alignment may produce different descriptions of diverged sequences depending on the relative position of the read.
Alignment works best when the sequences we are aligning are similar to the reference.
Increasing divergence requires greater computational expenditure to overcome reference bias.

We can avoid reference bias by working on pure assemblies generated only from the sequencing data in our experiment and unguided by any prior information.
Doing so can be rigorous, but comes at a significant cost, especially when the assembly algorithm requires us to load all the sequencing data into memory simultaneously.
We will require much higher coverage to obtain the same level of accuracy in our assembly as we will have when resequencing, and our read lengths will limit the length of contiguous sequences we can infer.
Virtually all assembly algorithms lose information about their source reads through the process of assembly, and this information must be somehow reconstituted if we wish to apply downstream algorithms to the input data in the context of the output of the assembler.
Although many approximations exist, no practical algorithm allows us to jointly consider all the samples in a large sequencing experiment in the context of an assembly generated from their reads.

Genome assemblers frequently use a compacted graphical representation of their inputs to support the algorithms they used to derive contiguous sequences from the fragmented input they are given.
These data structures are typically bidirectional graphs in which nodes are labeled by sequences and edges represent observed linkages between sequences.
If constructed from a set of reads that fully cover the genome, it can be shown that such a graph contains the genome which has been sequenced.
In effect, the assembler works to filter the edges from the graph and un-collapse repeats in order to establish a sequence assembly.

My contribution in this work is to resolve the problem of reference bias by repurposing and extending this data model to build a pangenomic reference system.
By patterning its structure on the data structures used in assembly, I resolve the issue of reference bias by enabling the construction of reference systems that fully incorporate data from all the samples in our analysis without bias towards any particular sample.
This supports a uniform and coherent basis space for sequence analysis.
By building a conceptual framework and data structures that enable resequencing against this structure, we can mirror the patterns and workflows that have already been explored in resequencing.
This allows us to retain the benefits of out of core and distributed analysis even while we resolve the issue of reference bias.
By recording reference sequences, or other sequences of interest as paths through this graph I provide anchors for existing annotations and positional systems within the pangenome.
I term these bidirectional sequence graphs with paths \emph{variation graphs}.

In this chapter I will provide background context for my work.
I will cover in detail the history of DNA sequencing methods, assembly algoritms, and the development of reference genomes and their use in resequencing.
Finally I will review similar data models, both proceeding and contemporary in development to mine.
In the remainder of the work I will describe the development of data structures and algorithms that allow the use of variation graphs as a universal reference system for unbiased genome inference, and finally, I will describe a series of experimental results that support their utility as such.

\section{Genome inference}

% What do we mean by genome inference
% half page
Not two centuries have passed since the first experiments that demonstrated the existence of genetic material \cite{mendel1866versuche}.
In the first part of the twentieth century, these ideas about heredity grew into the core of a modern sythesis linking biological micro and macroevolutionary theory into the quantitative basis of genetics \cite{huxley1942evolution}.
It was understood that DNA encoded the information that gave rise to biological structures \cite{avery1944studies}.
The discovery of the structure of DNA in the 1950s \cite{watson1953molecular} made clear that the information stored in the genome was essential and the physical basis for its faithful transmission from generation to generation was understood.
This observation, coupled with the sequencing and synthesis of proteins, which demonstrated that they had distinct polymeric chemical identities \cite{sanger1951amino} led to Crick's postulation of the ``central dogma'' of biology \cite{crick1958protein}.
Simply stated, the ``dogma'' argues that in living systems information is transcribed from DNA to RNA and ultimately translated into proteins, which then guide and structure the cell and thus living organisms.
The central dogma made the need to determine the specific sequence of the genome clear, and over the following decades a series of projects scaled up the throughput and fidelity of DNA sequencing until \emph{genome inference} became a practical and everyday reality in biology.

\subsection{Reading DNA}
% Genome sequencing techniques
% half page

The quest to sequence genomes began with arduous and sometimes dangerously radioactive experimental techniques, in which years of researcher time could be spent in obtaining sequences of tens of bases from partly-characterized sources.
It has then progressed through three distinct phases.
In the first, these early laboratory techniques gave way to automated sequencing using chain terminator chemistry, and related techniques were ultimately used to generate genome sequences for many model organisms, albeit at high costs.
In the second phase, \emph{multiplex} sequencing reactions were used to miniaturize the chain terminator reaction and observe its progression using fluorescent imaging or electrical sensing, evoking a drop in cost per sequenced base of many orders of magnitude, and simplifying library preparation steps dramatically by sequencing \emph{in vitro} clones of individual molecules.
The third wave of development has been characterized by two techniques which allow realtime observation of single DNA molecules.
These produce enormously long read lengths that are limited by the molecular weight of input DNA, but produce readouts with extremely high per-base error rates.
In theory, DNA could be directly input into such systems, obviating the need for library preparation, but in practice this has yet to be realized.
Supporting the second and third wave are methods that allow for haplotype-specific sequencing and the observation of long range structures in the genome.

\subsubsection{The old school}
% Sanger sequencing, BACs, etc.

In the 1970s a group led by Walter Fiers published the first complete gene \cite{jou1972nucleotide}, and then genome sequence \cite{fiers1976complete} from the MS2 bacteriophage using extremely laborious digestion and 2-dimensional gel electrophoresis techniques to sequence RNA based on work by Fredrick Sanger and colleagues \cite{sanger1965two, adams1969nucleotide}.
To avoid the limitations of digestion based assays, Ray Wu and colleagues developed a sequencing technique based on the blockage of DNA polymerization with radiolabeled nucleotides, which allowed them to sequence short fragments of lambda phage DNA using a similar 2D gel electrophoresis step to obtain a sequence readout \cite{wu1972nucleotide, padmanabhan1974chemical}.
Subsequently, Sanger, who at this point had developed techniques for sequencing RNA, protein, and DNA, developed a reliable DNA sequencing method based on the same DNA polymerization chain-termination concept \cite{sanger1977dna}.
By dividing the sequencing into four reactions, one for each DNA base, and sorting the resulting DNA fragments in parallel on a acrylamide gel, this sequencing method allows for a linear readout of DNA sequence.
Optimized and implemented using fluorescent chemistry \cite{strauss1986specific}, this approach, now known as Sanger sequencing, became the foundation of the first commercial sequencing machines in the 1980s, which automated DNA sequencing and allowed for dramatically higher throughput, enabling the establishment of the first large genome assemblies in the 1990s.

Sanger sequencing was the workhorse standard of biology for nearly 30 years, from the late 1970s until the mid 2000s.
Its read length is limited by the reaction efficiency required to obtain a fraction of terminations at every base in the sequence.
In practice, reads of 500 to 1000 base pairs can be reliably obtained, which are sufficient to cover a large portion of a gene.
With clonal DNA as input the per base accuracy of the method is expected to be extremely high, as each base readout reflects the termination of large numbers of molecules \cite{castiblanco2013primer}.
This high accuracy ensures that it remains important for validation despite the development of many new, and often cheaper sequencing methods \cite{sikkema2013targeted}.

Sanger sequencing requires clonal input DNA to perform accurately, as heterogeneity in the input DNA library can produce muddled signals that rapidly become uninterpretable.
With single nucleotide polymorphisms (SNPs) the problem is minimal, and SNPs my be readily called from the sequencing traces produced by automated dye-terminator sequencers.
However, insertions and deletions (indels) will cause a loss of phase in the sequencing and ultimately a failure to readout the DNA sequence from the trace \cite{tenney2007tale}, a problem which was still encouraging algorithm development nearly 40 years after the initial publication of the sequencing method \cite{hill2014poly}.
Consequently, in order to sequence whole genomes, which are often heterozygous, certain laboratory techniques were developed to allow the segregation of clonal DNA as a substrate for sequencing.
The most common methods used as the technique was applied to sequencing large genomes are bacterial artificial chromosomes (BACs) and their equivalent in yeast (YACs) \cite{monaco1994yacs}.
By isolating and clonally propagating DNA fragments of tens to hundreds of kilobases, these techniques allowed Sanger sequencing to be used to sequence genomes much larger than its maximum read lengths.
Similarly, the effective read length could be increased by using ``mate pair'' techniques, in which the ends of a longer molecule would be sequenced \cite{schmitt1996framework}.
However, to do so would require the development of computational techniques to automatically assemble the resulting sequence fragments into the BACs and ultimately chromosomes from which they were derived \cite{myers2000whole}.

\subsubsection{``Next'' generation sequencing}

In the late 1990s and early 2000s, several groups began exploring an alternative sequencing modalities.
In the ultimately dominant one, clonal DNA arrayed on a surface are directly sequenced using fluorescent imaging.
Sequencing progresses through the sythesis of the second strand of each of the molecules, and so these techniques are typically called ``sequencing by sythesis.''
This modality allowed for a massive parallelization of the sequencing reaction, and has resulted in a dramatic reduction of cost.

In 2003 George Church and colleagues demonstrated that individual sequences could be read from polymerase colonies or ``polonies'' suspended in an acrylamide gel \cite{mitra2003fluorescent}.
These polonies were generated as the acrylamide gel prevents the diffusion of the PCR reaction used to amplify single DNA fragments embedded within it.
Subsequently the sequence of each polony could be read out by synthesizing the second strand using fluorescently labeled blocking dNTPs, imaging in four colors with the fluorophores attached to determine the current base, and then unblocking the reaction with UV light or a reducing agent.
Contemporaneously, a related sequencing by synthesis method, which is now known as Illumina dye sequencing, was developed by Shankar Balasubramanian and Solexa (later acquired by Illumina) \cite{balasubramanian2004arrayed, bentley2008accurate}.
Rather than polymerase colonies embedded in an emulsion or gel, Solexa's technology relied on ``bridging PCR'', in which the polymerized clones of a particular fragment were locally hybdridized to an adapter-bearing surface of a flowcell.
Through successive PCR steps, a micrometer-wide spot of clonal DNA is cloned around the original position of its base molecule.
Reversible terminator chemistry \cite{canard1994dna} and fluorescently labeled dNTPs are used to observe the sequence of the DNA molecule.

Similar efforts to reduce sequencing costs yielded a series of novel technologies, most of which have seen little use after their development.
Church's group focused on a hybridization based sequencing protocol proceeded by an emulsion based polony PCR step \cite{shendure2005accurate}.
Initally the group attempted to commercialize an open source sequencing device (the Polonator)\footnote{My interest in open source projects, developed while an undergraduate studying the social sciences, led me to work in this device. The project introduced me to biology, bioinformatics, and DNA sequencing, which have attracted my interest and effort ever since.}, but the technology was ultimately brought to market by Applied Biosystems in their SOLiD (Sequencing by Oligo Ligation Detection) sequencing device.
In this technique, random oligonucleotides where a single base (or pair of bases) is known and correlated with an attached fluorophore are arrayed over clonal DNA fragments in order to read out the bases of the sequence.
Although in principle this method may achieve lower per base error rates due to redundant observations, the technology was plagued by artifacts related to contextual bias in the hybridization and ligation reaction as well as difficulty in normalizing the random probe set \cite{challis2015distribution}.
In ion semiconductor sequencing (as implemented in the IonTorrent platform) direct observation of pH changes at a fixed spot rather than fluorescent imaging were used to determine DNA sequences \cite{rusk2010torrents}.
It continues to see use in some diagnostic laboratories due to its rapid operation and favorable read length.
454 Life Sciences' ``pyrosequencing'' implementation allowed for a different kind of sequencing by synthesis in which a luciferase reporter assay could be used to track the progression of synthesis \cite{margulies2005genome}.
This technology was subsequently used to generate the first whole genome human sequence using massively parallel sequencing techniques \cite{wheeler2008complete}.
Both pyrosequencing and ion semiconductor sequencing suffer from homopolymer errors, as both progress through the addition of a single dNTP type at a time to the reaction.
As such it is necessary to infer the number of identical bases that have been incorporated, which can in practice be very difficult.

In contrast, Illumina's sequencing protocol has a superior error profile, with low overall per base error rates and only a handful of context specific error types, virtually all of which may be detected when sequencing both DNA strands \cite{allhoff2013discovering}.
In retrospect, it is unsurprising that it has achieved near total dominance in the field.
The vast majority of sequencing data produced in the 2010s comes from Illumina sequencers.
Illumina's sequencing technology is characterized by short reads (<250bp) with lower ($\approx 99.5\%$) per-base accuracy than Sanger sequencing.
Although the read length has been increased by optimization of the technology, physical limits prevent obtaining longer read lengths, as the per-base reaction efficiency of the sequencing by sythesis reaction causes a de-phasing of the reaction and thus degredation of the fluorescent signal with the progression of the cycles of second strand base addition.
As each read is derived from a single input template, problems with heterozygosity seen with Sanger sequencing can be resolved, and whole genomes may be directly sequenced and assembled.
However, the short read length places fundamental limits on the utility of the method for this purpose as even when using pair-end sequencing it is unable to resolve repeats greater than the read length \cite{alkan2010limitations}, and it is most-commonly analyzed via resequencing.

Alongside the development of second generation sequencing methods, an array of laboratory techniques and library preparation protocols have developed that augment the information that may be obtained solely from shotgun DNA sequences.
These methods can allow for the direct observation of large haplotypes and three-dimensional structures in the DNA, both of which are relevant for inferring whole genomes.
Shortly after Illumina released its sequencing technology, Moleculo developed an emusion-PCR based chemistry in which specific tags would be added to DNA sequences from a given haplotype.
By collecting short reads from a given tag together and assembling them, a set of synthetic long reads could be established.
10X Genomics commercialized a similar library preparation step, and focused on providing raw tag information that could be used downstream by an array of haplotype-resolution and assembly tools \cite{mostovoy2016hybrid}.
The single template aspect of Illumina paired end sequencing allows longer contiguous DNA reads to be obtained by merging partly-overlapping read pairs computationally \cite{magovc2011flash}.
Although this introduces error it can improve assembly contiguity and is often used when the library design permits it.
Single-cell DNA template strand sequencing (strand-seq) can be used to obtain reads from only one half of the chromatids in a single cell \cite{falconer2012dna}.
In strand-seq, a cell culture is treated with treated with bromodeoxyuridine (BrdU) and arrested after a single cell division.
The newly-synthesized strand of each chromosome will have incorporated BrdU, which can be cleaved using UV induced photolysis.
This treatment shatters the nascent strand into tiny fragments, allowing the selective sequencing of the other strand, which can then be used to improve the resolution of complex structural variation even though the direct observation is done with very short reads \cite{porubsky2016direct}.
The Hi-C method \cite{lieberman2009comprehensive} uses bisulfite treatment to generate read pairs that are likely to physically co-locate \emph{in vivo}, thus enabling the mapping of long range DNA and chromatin interactions.
It may be combined with other sequencing information to obtain estimates of the syntenic ordering of contigs produced by assembly \cite{ghurye2018integrating}.
This technique which has already been used to obtain \emph{de novo} reference quality genomes in several difficult sequencing projects including amaranth \cite{lightfoot2017single}, \emph{Aedes aegypti} \cite{dudchenko2017novo}, and the domestic goat \cite{bickhart2017single}.

\subsubsection{Single molecules}

All previously described sequencing techniques have dependent on the observation of pools of molecules.
As a result such methods benefit from amplification of DNA, which adds complexity and a potential source of error to DNA sequencing.
They also suffer from phasing and stepwise reaction efficiency effects, which fundamentally limit the maximum length of an accurate read.
By avoiding these issues, a method to sequence single molecules accurately would theoretically allow longer read lengths and more rapid sequencing.
The development of such a method has not been easy, and efforts have been continuously underway throughout 2000s and 2010s.
Although they have not yet achieved lower error rates or per base costs than the dominant Illumina method, due to their dramatically longer read lengths and flexible sequencing chemistries, the commercial sequencing platforms based on this principle are rapidly defining a new technical phase of genome inference.

In the mid 2000s, Helicos commercialized a single molecule sequencing system based on the same fluorophore-labeled, blocking sequencing by sythesis model developed by Illumina.
However, instead of observing a colony of millions of identical reactions per read, they observed a single one.
Although innovative, this daring technique suffered from numerous technical problems related to photdamage of the DNA, polymerase, and fluorophores.
It required physically massive hardware in order to enable the microscopy required for reliable of single molecules.
Ultimately, the short read length, high cost, error rate, and low throughput of the method relative to Illumina's prevented its use, and few genomes were sequenced with the method outside of its development \cite{harris2008single}.

By utilizing zero-mode waveguides (ZMWs) to observe DNA polymerase in realtime, Pacific Biosciences (PacBio) generated the first realtime single-molecule sequencing system \cite{eid2008real}.
In this platform, DNA polymerase is immobilized in sub-diffraction size, picoliter detection volumes at the bottom of wells formed in aluminum on a glass slide \cite{korlach2008selective}.
Single stranded DNA and fluorescently-labeled dNTPs are added to the buffer above the ZMWs.
As synthesis progresses, the fluorophore attached to the DNA base that is being incorporated will tend to remain inside the ZMW longer than would be expected due to random diffusion of the dNTPs.
Thus, the series of incorporated bases in each ZMW well may be read out as a sequence of fluorescent pulses, allowing the synthesis of the second strand to be observed in real time.
The main sources of error stem from difficulty in perfectly observing the series of fluorophores pulled into the well, and also not discriminating random ZMW occupancy from polymerization-mediated occupancy, which tend to result in insertion errors.
The base-level error rate of sequencing is high, up to 15\% and subtle context dependent biases do exist \cite{ono2012pbsim}, but due to their genesis in Brownian motion, the errors themselves may be considered as almost perfectly random in analysis \cite{ross2013characterizing,myers2014efficient}.
The configuration of the sequencing system allows for unique chemistries, such as via the sequencing of circular DNA molecules, which can provide dramatic reductions in error at a cost of read length.
PacBio's system has taken much of a decade to develop into a reliable sequencing platform, but in recent years has become a foundational technology in genome sequencing, with many recent genome assemblies completed using it \cite{rhoads2015pacbio}.

The idea that electrophoresis of DNA through nanometer scale pores might allow the direct sequencing of DNA was first postulated in the late 1980s by David Deamer and others \cite{deamer2016three}.
While the sequencing model itself is among the simplest ever proposed, it would take twenty-five years of work \cite{kasianowicz1996characterization,purnell2008nucleotide} before the technique was brought to market by Oxford Nanopore (ONT) \cite{mikheyev2014first} and used to fully sequence genomes \cite{loman2015complete, jain2018nanopore}.
In nanopore sequencing, a DNA strand is transported through a nanometer pore by electrophoresis.
As the DNA molecule traverses the pore, the specific DNA bases within the pore effect characteristic changes in the electric currenty density within the pore.
By measuring the changes in current over time, it is possible to infer the sequence of the DNA molecule.
Due to context and history-dependent effects that distort the signal, the measured patterns in the current flux must be interpreted by sophisticated models that have been trained to convert the traces to DNA sequences \cite{david2016nanocall}.
In practice nanopore sequencing has the highest error rate of any commercially available sequencing method.
The error process is complex, being related both to the intricacies of pore/DNA interaction and by patterns of error in the base calling algorithm.
As with PacBio, it approaches 15\%, and similarly this has improved continuously throughout the development of ONTs sequencers.
Nanopore sequencing also obtains the longest reads of any sequencing technology, with maximum read length apparently limited solely by the molecular weight of the DNA library.

%, single molecule ONT/PacBio (3rd gen)

\subsection{Genome assembly}
\label{sec:genome_assembly}

Due to technical limits that are unlikely to ever be fully eliminated, DNA sequence reads are rarely able to cover the entire genome of an organism.
This means that in many cases, the best sequencing data possible is a set of random reads sampled across the genome. 
The standard response to this limitation is whole genome ``shotgun'' sequencing, wherein the genome is fragmented, perhaps by sonication or enzymatic digestion, and the resulting fragments are sequenced and then reassembled using computer programs \cite{gardner1981complete, sanger1982nucleotide}.
This process necessitates a reconstructive step in which the information obtained from the sequenced fragments is reassembled into the whole genome from which they arose.
This process is known as \emph{assembly}, and computer algorithms implementing it have been used when inferring genome sequences since the generation of the first whole genome sequence for bacteriophage $\varphi$X174 in 1977 \cite{sanger1977nucleotide, staden1979strategy}.

The earliest assembly algorithms have come to be known as ``overlap-layout-consensus'' (OLC) algorithms, due to their three-phase strategy.
They first establish a set of head to tail overlaps between reads (overlap), an $O(N^{2})$ order problem, where $N$ is the length of the sequence reads.
This might be done by exact string matching or by sensitive pairwise alignment of reads likely to map together to each other \cite{huang1992contig}. 
Then, these overlaps are used to establish an estimate of the ordering of the reads (layout).
The layout is then used to generate a consensus sequence through heuristics or dynamic programming over the layout \cite{kececioglu1995combinatorial}.
This final phase is equivalent to the multiple sequence alignment (MSA) problem, although instead of generating an MSA as output methods would typcially take the consensus sequence, as the objective was to reconstruct a singluar representation of the input genome.
Assembly errors were frequent, which necessitated a time-consuming manual ``finishing'' step in order to resolve ambiguity, resulting in the development of software to aid this process \cite{gordon1998consed}.
The OLC assembly approach was utilized by genome projects for the following twenty five years, and was employed even in the completion of the public Human Genome Project (HGP), where BAC clones of 150kb fragments of the genome were sequenced, initially assembled by algorithm and finally manually finished into the ``golden path'' that would become the reference genome \cite{international2001initial}.

In principle, the assembly process could be fully automated, but as late as the early 1990s this frequently not seen as feasible due to the lack of reliable algorithms \cite{mahy1991sequencing}.
The improvement of OLC algorithms eventually met the challenge, yielding methods such as PHRAP \cite{green1999phrap} (a quality aware assembler that saw extensive use downstream of Sanger sequencers), TIGR \cite{sutton1995tigr} (which was used in the generation of the first assembly of a free living organism, the 1.8Mpb genome of \emph{Haemophilus influenzae} \cite{fleischmann1995whole}), GigAssembler \cite{kent2001assembly} (which was used by the HGP), and the Celera assembler \cite{myers2000whole,miller2008aggressive} (which saw extensive use in the generation of early large whole genome assemblies in the late 1990s and early 2000s, including the privately funded genome project \cite{venter2001sequence}).\footnote{This project apparently still relied on data produced by the HGP \cite{waterston2002sequencing}, but the significance of this reliance was disputed by researchers involved in the private project \cite{myers2002sequencing}, who argued that the manner in which they used the public sequences avoided contamination by manual finishing done by the HGP.}
The process implemented in the Celera assembler (including repeat masking) has remained essential to the genome assembly problem until the present.

In 2005, Myers formalized an idealized version of the assembly problem in the \emph{string graph} data structure \cite{myers2005}, which is a sequence graph induced from the overlaps in a set of shotgun sequencing reads.
This model demonstrates that repeats greater than the length of a sequence read will collapse into singular regions of the graph, while unique sequences will form loops between different repeat classes that flank them.
The string graph can be shown to represent the full information available in the input sequence data, and some of the most-popular algorithms for assembling short read sequencing are built around an induction of the string graph via the construction of the FM-index \cite{ferragina2001experimental} from Illumina read sets \cite{simpson2010,simpson2012efficient,li2015fermikit}.
If not using compressed data structures and low-error reads, the repeats are often irresolvable and may be masked from the assembly process to improve performance on the tractable non-repetitive regions of the genome, which is a strategy promoted and still employed by Myers \cite{myers2014efficient}.
Canu and FALCON, which to some extent stand as contemporary implementations of the Celera assembly process, are among the best-performing assemblers for noisy single-molecule sequencing data that is the mainstay of current genome assembly projects \cite{chin2016phased,koren2017canu}.
These and other similar methods have shown that recent long reads can be used to fully assemble genomes directly from shotgun sequencing and without human finishing \cite{loman2015complete,jain2018nanopore}.

The repeat problem has been tackled in various ways, but one of the most enduring solutions resolves the issue through the reduction of the assembly overlap graph to a de Bruijn graph (DBG) \cite{pevzner2001eulerian}.
In this approach, the read set is fragmented into all subsequences of reads of a given length $k$, and a graph is constructed where $k$mers label nodes and overlaps of $k-1$ between successive $k$mers induce edges representing linkages between them.
By reducing the complexity of the representation of the read set, this model greatly improves the runtime and memory requirements of assembly and has been essential to modern assembly methods \cite{zerbino2008velvet,simpson2009abyss,iqbal2012novo}.
Still, significant effort has been required to improve the memory usage of these methods, which ultimately has generated the most memory-efficient assembly methods for short read sequencing data, with the best-performing using techniques like bloom filters \cite{chikhi2013space}, succinct DBGs \cite{bowe2012succinct,li2015megahit}, and minimizer partitioning \cite{chikhi2016compacting} to generate a compressed representation of the DBG.
DBG based assemblies suffer from the loss of information induced through the $k$merization of their input, causing a reduction in assembly contiguity \cite{earl2011assemblathon}, although in practice this can be mitigated by reconsideration of the input reads and read pairs \cite{butler2008allpaths}.
They also are applicable only where the sequence error rate is low enough for overlapping reads to be expected to have exact matching $k$mers of the appropriate size (typically, $k \in [20 \ldots 50]$ base pairs), and as such cannot be applied to third generation single molecule sequencing due to its inherently high error rate.

Many of the sequencing methods I have described above are still in use today.
Each popular method, as it fades from use, remains relevant in a niche area where its particular properties provide it a comparative advantage.
As a result, we are not presented today with a single ideal sequencing method, but a menagerie of approaches, each with its own limitations and benefits, and current assembly pipelines require thoughtful design to incorporate these myriad sources of information.
It would appear that in order to use these many technologies to generate the best-possible assemblies we must bring them together in a single model \cite{chaisson2018multi}.
A current development in assembly focuses on the design of a common interchange format for which to organize such assembly processes, which has been implemented as the GFA v1 and v2 formats.\footnote{\url{https://github.com/GFA-spec/GFA-spec}}
This file format and the data model it implies is an essential link between the work that I present in this thesis and the problem of genome inference.

\section{Reference genomes}

As we have seen in the previous section, obtaining a single genome sequence \emph{de novo} is an arduous task, and remains a complex problem.
The result is a valuable object which can be used to lower the cost of subsequent analyses and enable direct whole genome comparisons which provide a full perspective on the genetic relationship between two individuals or species.
The need for reference genomes is clear, and they are collected in open public databases to allow their dissemination and use by researchers.
NCBI's RefSeq release 89 of July 13, 2018 contains some 81,345 organisms\footnote{\url{https://www.ncbi.nlm.nih.gov/refseq/}}, although it should be noted that only a handful of these genomes are eukaryotic.
Recent developments in long read, single molecule sequencing have enabled great decreases in the cost and complexity of generating high-quality genome assemblies, supporting a recent project to generate reference quality genomes of ten thousand vertebrates \cite{genome2009genome,koepfli2015genome}.

The reference genome serves as an anchor for annotations that describe sequences and regions of interest within the genome, such as genes, exons, chromatin structures, DNA interacting proteins, and genetic variation \cite{sherry2001dbsnp,quinlan2010bedtools,encode2012integrated}.
An established reference genome can serve as a conceptual foundation for the communication and interpretation of scientific results \cite{kent2002human}, and is seen as essential for collaboration and the development of a genome research community in a particular organism \cite{smith1998functional,cherry1998sgd}.

Traditionally, references are seen as singular haploid models of the genome, and represent only a single version of each genomic locus.
This conceptual simplicity is a core feature of their public use.
Although the issue of genetic diversity has always been appreciated by those who work with genomes, expediency has encouraged the use of linear models for reference genomes.
For instance, within the HGP members of the consortium could observe diversity within the BAC clones that they had sequenced from different human donors, and initated a debate about the inclusion of heterozygosity in the reference itself.
Ultimately, the twin needs of ``completing'' the reference sequence and maintaining a simple conceptual system (the haploid sequence became known as the ``golden path'') encouraged the publication of a simple linear reference,\footnote{Personal communication with David Haussler.} and it was only with the most-recent release of GRCh38 that a concept of alternative locus and variation was added to the human reference genome \cite{schneider2017evaluation}.
%and to my knowledge this is the only reference genome project in which such a step has been taken 

% history of reference sequences
% use of the reference sequence in analysis
\subsection{Resequencing}

Due to the high cost of obtaining error-free, full length genomes, standard practice will use the best genome assembly for a given organism as a \emph{reference genome} when analyzing the sequences of other organisms from the same species.
To do so, the genomes of the other individuals do not need to be fully assembled, and instead shotgun sequencing libraries from these new individuals may be aligned back to the reference to find small differences between the genomes.
To distinguish it from whole genome sequencing and assembly, this process is known as \emph{re}sequencing.
In effect, it provides a low-cost and low-effort route to the inference of a new genome, with the caveat that the inferred genome must be expressed in terms of the reference one.
Resequencing typically is understood as having two phases.
In the first phase, reads from the sample or samples under study are aligned against an appropriate, genetically similar, reference genome.
In the second phase, the aligned reads (alignments) are processed together locus by locus to determine allelic variation within the samples relative to the reference genome.

\subsection{Sequence alignment}
\label{sec:sequence_alignment}

An \emph{alignment} expresses one sequence in terms of a set of positions, edits, and matches to another.
Algorithms to determine the most plausible alignment between a pair of sequences have as long a history as sequencing itself, with the first significant attempts to algorithmically assess sequence homology and divergence between protein sequences arising in the 1960s with Fitch's method for homology detection \cite{fitch1966improved}.
This method involved the comparison of many subsequences of two sequences to be compared, so as to account for insertions and deletions, a fact which Needleman and Wunsch appreciated in their development of an O(NM) time algorithm for the global alignment of sequences in 1970 \cite{needleman1970general}.
Given strings to compare of length $N$ and $M$, the algorithm builds a $M \times N$ matrix in which any possible full length alignment between both sequences can be expressed as a path through a series of cells.
To determine the most-likely path, Needleman and Wunsch apply a recurrence relation dependent on the characters at each pair of positions in the strings and the values of the neighboring cells.
The relation is designed such that a match corresponds to the shortest (diagonal) path through the matrix, and insertions and deletions correspond to horizontal or vertical movements.
This implementation of dynamic programming (DP) \cite{bellman1952theory} breaks the alignment problem into the successive application of all the sub-alignment problems.
For each cell, the score is given as the maximum of: the score of cell to the diagonal plus a bonus if the corresponding sequence characters are the same and minus a penalty if they are different; and the scores of the cells above and below minus a penalty corresponding to the weight given to an insertion or deletion.
Finally, we determine the optimal path beginning from the opposite extreme cell of the matrix from where the scoring began, in which we walk back through the successive maximum scores until reaching the opposite extreme corner of the matrix.
This traceback encodes the alignment, which is most-simply represented as a vector of pairs of matched bases in each sequence.
It can be shown that provided full evaluation of the dynamic programming problem, the optimal alignment is obtained given a set of scores parameterizing the recurrence relation.

This alignment algorithm is known as a ``global'' alignment algorithm, in that the alignment covers all bases of both sequences.
In practice, this level of comparison is not always needed, and it can be advantageous to obtain only the optimal sub alignments between sequences.
Smith and Waterman provided a clean formalization of the algorithm of Needleman and Wunsch, modifying it to prevent negative scores, a step which allowed them to produce the optimal ``local'' alignments \cite{smith1981comparison}, ignoring regions unlikely to contain significant homology.
The algorithm, further refined by Gotoh \cite{gotoh1982improved} to enable affine gaps\footnote{In affine gap schemes the cost of a gap per base decreases as its length increases. Such a scheme approximates the $\zeta$-distributed excursions of a particle under brownian motion, which structure the length of insertions and deletion mutations observed in nature.} and computation in $O(MN)$ time, is today one of the most important in genome analysis.
The amount of work on this topic is considerable, and the subsequent decade yielded numerous modifications of the basic alignment concept, for instance reducing the memory bounds to $O(N)$ through a divide and conquer approach \cite{myers1988optimal}, and further explorations of affine gap scoring schemes \cite{altschul1986optimal,gotoh1990optimal}.
Numerous following works have offered improved implementations, using vectorized instructions to improve the runtime of the algorithm \cite{farrar2007striped} and heuristics to selectively evaluate only part of the DP matrix \cite{suzuki2017acceleration}.
However, such changes were not sufficient to enable alignment against large sequence databases.

$O(MN)$ algorithms for sequence alignment are impractical when either $M$ or $N$ becomes large.
Naturally, as sequence databases grew and the size of sequenced genomes increased, heuristic strategies to efficiently reduce the alignment problem size were introduced.
When aligning a short sequence against a large database we expect to obtain a sensitive alignment, but provided sufficient homology between the sequence and the database it is unlikely that we need to evaluate the full problem using an algorithm like Smith-Waterman-Gotoh (SWG).
By indexing either the query or target set of sequences to efficiently obtain patterns of exact matches, candidate sub-regions of both can be isolated and submitted to more sensitive alignment.

This strategy was implemented in the mid to late 1980s in the FASTA \cite{pearson1988improved} and BLAST \cite{altschul1990basic} alignment algorithms.
FASTA greatly accelerates pairwise alignment by a pattern which remains common in the current generation of sequence aligners.
It first uses a seeding step that simulates the full DP matrix between the query and target, using short $k$mer seeds to establish the longest matching subsequences.
A small number of candidates are enumerated and evaluated using a banded SWG algorithm.
In contrast, by default BLAST implements a fully heuristic alignment process based solely on the $k$mer seeds and ungapped alignment.
This is much faster than FASTA but can perform slightly worse in the context of highly divergent sequences.
BLAST's heuristic alignment comprises a speedup of many orders of magnitude over full DP based algoritms at a minor cost to accuracy.
The popularity of BLAST in biology\footnote{The BLAST1 paper has been cited more than 70 thousand times as of August 2018.} is clear evidence of the importance of the alignment problem to all kinds of genomic analysis.
It is also evidence that minor losses in accuracy are acceptable given the cost of sequence analysis in large data sets.
Jim Kent's Blast-like alignment tool (BLAT) indexes the target set with non-overlapping $k$mers and queries all $k$mers in the reads, yielding a method that is less sensitive but several orders of magnitude faster than BLAST \cite{kent2002blat}.
The increased performance was a necessity when working with the scale of data generated in the HGP.

As reliable commercial second-generation sequencing systems became available the rate of sequence data acquisition growth rapidly outstripped the rate of improvement in computing performance \cite{leinonen2010sequence,kodama2011sequence}.
This necessitated further improvements in the computational cost of sequence alignment.
The most-widely used of these methods focused on the increasingly prevalent problem of aligning short reads to reference genome type sequence databases.
Due to the high quality of the reference sequence, low error rate of the short ($\leq$100bp) reads, and low nucleotide diversity of humans (where $\theta \approx 1^{-3}$), algorithms that focused on exact string matching had great success in tackling the resequencing problem in this context.
Much like BLAST and BLAT, the first wave of aligners capable of indexing the human reference genome and aligning short reads to it efficiently utilized exact $k$mer matching via hash tables followed by local alignment via heuristic or DP methods \cite{li2008soap,lee2014mosaik,li2008mapping}.
Substantial improvements would be yielded by the development of aligners based on contemporary developments in compressed data structures.

The suffix tree \cite{weiner1973linear} encodes all suffixes of a sequence in the structure of a tree such that the suffixes may be enumerated by a depth first search (DFS) of the tree.
This structure can be used to determine if a given sequence $q = c_{1}c_{2}\ldots c_{|q|}$ is present in the sequence from which the tree was built in $O(|q|)$ time.
Search begins at the root, progressing across the topology (edge or node) of the tree which is labeled with the next character until no further matches may be found.
By augmenting the tree with the sequence positions corresponding to each node, the search may also yield the positions of the exact matches detected within the target sequence from which the tree was built.
Suffix trees may be built in linear time and space relative to their input \cite{ukkonen1995line}, and support diverse algorithms for string comparison \cite{apostolico1985myriad}, such as whole genome alignment \cite{delcher1999alignment}, but they require relatively large amounts of memory per input base.
In practice they were superceded by equivalent data structures with better memory bounds such as the suffix array (which represents the tree in a linear array of suffix sort ranks) \cite{manber1993suffix}, and its compressible and dual sibling the BWT \cite{burrows1994block}.

By augmenting the BWT to enable the exact matching via emulation of the suffix tree, Paolo Ferragina and Giovanni Manzini developed the FM-index \cite{ferragina2000opportunistic,ferragina2004alphabet}.
Given a string $S$, the FM-index may be built using a series of transformations and auxiliary data structures that induce the suffix tree in the context of the highly-compressible BWT.
First, we add marker characters for the start and end with lexicographic order less and greater than the rest of the characters in the alphabet, e.g. $\#,S,\$$.
Then, we take all rotations of the sequence and sort them, yielding matrix $M$.
This sort has many similarities to the suffix tree, and for instance it can be seen that a lexicographically-ordered DFS through the suffix tree would enumerate the prefixes of the rotations of the sequence preceeding the terminal character $\#$ in the same order as they are sorted.
The suffix array $SA$ is given by the vector by the suffix ranks in the sorted order they occur in $M$, and is related to the ordering of characters in the first column $F$, while the BWT is given as the last column, $L$.
If the input text has repetitive patterns, then these will tend to result in runs in the BWT of the string that may be compressed using various schemes.
To emulate traversal of the suffix tree, it is sufficient to construct a function $LF(i)$ which maps between indexes $L[i] = F[j]$ which represent the same character.
This is done by the augmentation of the data structure with array $C$ such that $C[c]$ returns the number of characters in the text that are lexicographically smaller than $c$, and a function $rank(c,k)$ that yields the number of instances of $c$ in the prefix $L[1 \ldots k]$.
Now, $LF(i)$ may be defined as $C[L[i]] + rank(L[i], i)$.
By allowing the traversal of equivalencies between the suffix array and the BWT, $LF$ mapping enables ``backward search''.
Because rotations of $S$ have been ordered in $M$, a given pattern will occur as the prefix of a particular contiguous range of $M$.
We can find all occurances of a given pattern by maintaining a range in the suffix array within which a given pattern occurs, beginning at $s$ and ending at $e$, which we initially set as $s=0$ and $e=|L|-1$.
To search for a pattern, we obtain $s' = C[c] + rank(s,c) + 1$ and $e' = C[c] + rank(e,c)$, walking backwards through the characters $c$ in our query $q$.
Our search terminates when we complete our query or $e' - s' \leq 0$, which indicates that the previous step in the search did not match our target $S$.
The various matches of $q$ in $S$ may now be obtained by the positions in $SA[s \ldots e]$.
The addition of the longest common prefix (LCP) array on the sorted suffixes can be used to emulate all algorithms on suffix trees in this context \cite{abouelhoda2004replacing}, and in turn this enables $O(|q|)$ search of queries against $S$.
Of particular relevance to this work, when completing a phase of backward search we could use the LCP to traverse the suffix links in the suffix tree and continue with the next (partially overlapping) maximal exact match.

The FM-index became one of the core data structures used in modern short-read aligners such as BWA \cite{li2009fast} and BOWTIE \cite{langmead2009ultrafast}, which used backtracing alignment to directly align sequences to the suffix array encoded in the FM-index.
This backtracking search was impressively fast, and perfectly matched the needs of aligning short human sequencing reads to the reference genome, but had problems detecting indels (as these require exponentially more backtracks to infer) and did not perform well with increasing read length.
In response, the authors merged initial exact matching with a final DP step to yield ``long read'' capable aligners like BWA-SW \cite{li2010fast} and BOWTIE2 \cite{langmead2012fast}.
Further refinements of this concept yielded BWA MEM \cite{li2013aligning}, which uses a heuristic algorithm to determine ``supermaximal exact matches'' (SMEMs) and reseed ``sub matches'' within them using a bidirectional FM-index (the FMD index).
Due to its relative robustness to error and variation, the MEM concept has ultimately prevailed and as of the time of this writing BWA MEM could be seen as the industry standard method for aligning short reads to the genome.

Much of the new sequencing data was being generated for humans in medically-motivated genome wide assocation studies \cite{uk10k2015uk10k} or population survey projects like the 1000 Genomes Project (1000GP) \cite{1000Gphase1,1000g2015}.
The development of these methods was accelerated by an open, competitive spirit fostered during the 1000GP, whose primary sequencing data remains the largest completely publicly available data set, with more than 100TB of sequence data available for download from public URLs without any authentication.
There, project participants formalized the resequencing process by generating a series of data formats linking the various stages of analysis, including the sequence alignmen/map format (SAM) and its binary equivalent (BAM) \cite{li2009sequence} that is the standard output format for contemporary aligners.


\subsection{Variant calling}

As we have seen, DNA sequencing reads of all types contain errors, and genomes contain diversity.
This means we cannot infer a genome directly from a single read, and must aggregate sequencing information to infer the genomic state at a given locus.
In the context of resequencing, techniques to do this have come to be known as \emph{variant calling}.
The simplest methods for this process resemble the consensus step in OLC assembly, and could be implemented as heuristic filters on the mutually gapped alignment matrix of a set of homologous sequence reads \cite{koboldt2009varscan}.
A natural framework in which to do so is a Bayesian one, in which prior expectations about the genomic state are combined with the available data to generate a posterior probability that allows the propagation of information about uncertainty to downstream analyses.
Such a model allows the integration of various sources of information in addition to the sequence of the reads themselves, including the base quality (BQ), or machine-estimated probability of an erroneous base call, and mapping quality (MQ), which represents the aligner's estimate that the given alignment is a mismapping or ambiguous \cite{li2011statistical}.
Furthermore, collections of genomes could be modeled at the same time using expectations of population genetic and combinatoric patterns, for instance in a panmictic population under neutral selection the pattern of observed genotypes should match Hardy-Wineberg Equilibrium (HWE), and to have confidence in a given genotyping call, the evidence for variation should be stronger than our expectation of the rate of haplotype diversity in a given sample.

The earliest implementations of this technique focused on genotyping samples using expressed sequence transcripts (ESTs) \cite{marth1999general}, which were promoted by Celera and others as a techinque to focus genome analysis on coding sequences.
The efforts in the 1000GP generated strong competition to develop the best methods to work on short read data, yielding variant calling algorithms based on a variety of principles.
The simplest method would detect variation given pointwise SNP and indel descriptions directly from the alignments \cite{li2009sequence,gatk2011}.
However, this technique was shown to be susceptible to inconsistencies in the alignment process, and several groups developed methods that would reevaluate the alignments in a reference-independent manner in order to homogenize the representation of small variation.
These techniques became known as ``local assembly'' variant detection algorithms, and include the windowed haplotype detection implemented in freebayes \cite{garrison2012haplotype} as well as full local \emph{de novo} assembly based on de Bruijn graphs as implemented in Dindel \cite{albers2010dindel}, Platypus \cite{rimmer2014integrating} and the GATK's HaplotypeCaller.
In parallel, several whole genome \emph{de novo} assembly methods, including SGA and Cortex, were applied to the full data set, yielding variant calls that were expected to be unbiased due to the reference genome.
The final project results were merged into a jointly inferred population genome assembly using statistical phasing techniques \cite{browning2007,howie2011,delaneau2012}, which were themselves guided by genotype patterns presented in a number of data sources, not limited to sequencing and including genotyping arrays \cite{1000g2015}.
As they did for alignments, members of the 1000GP also developed a standard format for describing collections of resequenced genomes, including their genotypes and inferred haplotypes, which is known as the variant call format (VCF) \cite{danecek2011variant}.
All current methods for variant calling produce this format.

Due to the absence of a reliable truth set, traditional algorithmic design rather than machine learning had to be used to develop most of the algorithms.
The complexity of the problem and its relatively low computational cost relative to alignment encouraged the development of many competing solutions.
Subsequently, projects at Illumina (Platinum genomes) and NIST (Genome in a Bottle) have generated ``truth sets'' for variant calls matched to cell lines for which large amounts of sequencing data is publicly-available \cite{eberle2013platinum,zook2014integrating}.
These truth sets have then enabled the devolpment of ``universal'' variant callers using machine learning techniques \cite{poplin2017creating}.
It may be expected that this trend will continue as the number of fully sequenced genomes increases.

\subsection{The reference bias problem}

When inferring genomes using resequencing, the reference can be thought of as a kind of prior.
Short reads are insufficient to generate \emph{de novo} assemblies of reference quality, and this issue is exacerbated when they are used in resequencing, as the prior information provided by the reference is relatively strong and can distort our perspective on the genome we wish to infer \cite{sudmant2015integrated}.
Contemprary aligners operate on the principle of matching each sequence read to the linear reference, and differences between the read and the reference induced by both error and variation will tend to reduce the success of mapping.
As I will demonstrate later in this work, reference bias is most severe for larger variants.
However the bias towards the reference is relevant even for SNPs, a fact which adds great complexity to experimental contexts sensitive to slight changes in allele observation count, such as allele specific expression (ASE) quantification from RNA sequencing \cite{stevenson2013sources}, or in the context of short and high error reads as are common in the sequencing of ancient DNA \cite{zhou2017antcaller}.

Advances in sequencing technology can reduce reference bias in some contexts where long reads can be obtained.
For instance, a long read aligned against the reference will much more easily represent a structural variation than a short read.
As costs have continued to reduce in both second and third generation sequencing, it seems likely that such reads will continue to cost more per base than shorter reads from second generation technologies, and there will be a cost advantage to resequencing with short reads for the near future.
Nonetheless, reference bias remains even in a future in which all sequencing was completed with long, low-error reads.
As long as the reference is used as a basis space for analysis, it will be impossible to develop coherent representations of all sequences in a given cohort.
In effect, we cannot observe variation in sequences which are not in the reference unless we bring these sequences into communication with each other, and even the problem of establishing if large variants represent the same entity becomes complicated \cite{chaisson2018multi}.
We can use improvements in assembly methods, such as the linked DBG \cite{turner2018integrating}, to build space-efficient joint assemblies of populations of genomes.
But these approaches will not allow out of core analysis processes like those implemented in resequencing pipelines, and are unlikely to improve in efficiency by the many orders of magnitude required to consider applying them directly to sequencing from hundreds of thousands or millions of genomes.

%By laying out the history of the field in detail, I hope to have built a case that such a change is justified and likely to be useful in any likely future state of sequencing and genome inference.
%We have seen that no sequencing method is perfect, and techniques retain utility for long periods of time due to the numerous tradeoffs inherent in all sequencing methods.

%Using two passes over the data, we can allow full communication between all DNA sequences without 

% it is necessarily harder to see things when the become more divergent
% literature review of examples

\section{Pangenomes}

The reference is a prior guide to resequencging.
If it could represent variation within itself, then we could build a resequencing model that avoided the traditional problem of reference bias.
Following the completion of the 1000GP, participants have sought to use the population reference established by that project as an input to genome inference processes.
Rather than establishing a single linear reference genome, these methods base their analysis on a representation that contains some or all of the known variation in the species of interest.
In these approaches, the reference system becomes a \emph{pangenome}\footnote{``pan-'' from Greek $\pi\alpha\nu-$, meaning ``all'' or ``every''}, or data space representing all the genomes and their interrelationships.
The term was first used to describe the sequence information obtained from DNA and RNA for a cancer sample \cite{sigaux2000cancer}, but later became an important concept in microbiology as results from bacterial genome sequencing indicated extensive diversity between bacterial genomes \cite{tettelin2005genome,medini2005microbial}.
Due to horizontal gene transfer (driven in large part by the permissive sex lives of bacteria), mobile DNA in the form of viruses and transposable elements, and their enormous population sizes, the genome diversity of many prokaryotes is much greater than that seen in larger, complex organisms.
In microbial pangenomic theory, the main object of interest is the open reading frame (ORF) and its distribution across species in a clade\cite{vernikos2015ten}, with particular interest to classification of ORFs or genes into a gradient between those that are essential and found in every species (the ``core'' pangenome) to those that are found infrequently (the ``dispensable'' pangenome).
With reducing sequencing costs, the levels of diversity in eukaryotic genomes could be more easily appreciated, and in the 2010s evidence has rapidly accumulated that significant levels of large-scale variation occur in the genomes of many species, including humans \cite{sudmant2010,sudmant2015integrated,chaisson2018multi}, brewer's yeast \cite{yue2017contrasting}, and the fruit fly \cite{chakraborty2018hidden}.
%  Computational Pan-Genomics Consortium formed at a workshop held from 8 to 12 June 2015, at the Lorentz Center in Leiden

The increasing evidence that the non-reference genomic variation mattered even in a human or medical context motivated extensive discussion within a sub-project of the Global Alliance for Genomics and Health (GA4GH)\footnote{The GA4GH is an international consortium of researchers and genomics professionals chartered with the development of new genomics data formats and interchange systems \url{https://www.ga4gh.org/}.}.
At the beginning of my studies I participated in the GA4GH's reference variation task team (RefVar), which was led by Benedict Paten, David Haussler, and Richard Durbin.
The group had regular meetings where its members entertained proposals for new variation-aware genomic data models and discussed results obtained with software implementations of them.
By chance, a meeting of the GA4GH in June, 2015 in Leiden overlapped a conference held at the Lorentz Centre on ``Future Perspectives in Computational Pan-Genomics''\footnote{\url{https://www.lorentzcenter.nl/lc/web/2015/698/info.php3?wsid=698\&venue=Oort}}, whose participants were discussing ways to apply the concept of pangenomics to many problems in genomics.
Can Alkan, who had been invited to both meetings, brought members of the GA4GH's RefVar group to the concurrent workshop, where both groups presented on their work and ultimately joined efforts.
This exchange motivated members of the RefVar group to consider many alternative resequencing and genome modeling problems.
For the consortium, our software {\tt vg} became a template for the pangenomic resequencing concept that it would present in the paper resulting from the meeting \cite{computational2016computational}.
And in turn, the consortium imagined the missing pieces that would be required to fully enable a pangenomic reference system and allow common genome inference patterns using it.
Much of the work I will present in the latter chapters of this thesis follows the design developed by this group.

% various approaches to encoding the pangenome

% think through, with references, the various ways we can resolve reference bias
% extended
\subsection{On pangenomic models}

Much of this work relates to a particular model for encoding a pangenome. Here, I will briefly describe alternative models and justify the use of a graphical one which I present, while the remainder of the chapter will provide background on more-closely related graphical approaches related to my work.

Traditional techniques from microbial pangenomics have focused on cataloging the distribution of ORFs across bacterial species \cite{page2015roary}.
In this sense the pangenome is not so much a sequence-based object, but a matrix encoding the presence or absence of genes across the species of a given clade.
If we want to use pangenomic principles to resolve issues with resequencing, then we must take the concept of pangenome more literally, and build a representation that losslessly encodes genomes together with a focus on their sequence content.
In this perspective, the classical pangenome becomes a derivative product that we can produce using analyses on our sequence-oriented pangenomic reference.

We could then describe a sequence-aware pangenome with the whole genome assemblies of many species or individuals, in an unfolded pangenome in which all sequence homologies and relationships are implicit.
This naturalistic design is the simplest possible for a sequence-aware pangenome.
In principle the unfolded pangenome could resolve reference bias and allow simple augmentation of the pangenome with new data by adding new genome sequences, although this operation already exposes a weakness, which is that adding a new genome always adds all the sequence in that genome to our system.
Techniques from compressed data structures have been used to resequence short reads against the unfolded pangenome, as implemented in the CHIC aligner \cite{valenzuela2017chic}, but numerous problems have prevented the exploration of this modality.
We can use this work to understand some of the limitations of this approach.
The CHIC aligner employs an indexing strategy which uses Lempel-Ziv (LZ77) \cite{ziv1977universal} parsing of the pangenome to generate a ``kernel'' sequence encoding the pangenome that may be indexed and used by a standard short read aligner (the authors use BOWTIE2).
In effect, the seeding step is aware of alternative sequences embedded in the pangenome, but the local alignment step is ultimately run against the linear reference.
It is worth noting that an approach which aligned directly to the unfolded pangenome would not be able to correctly estimate MQ, as the reference does not encode any model about the relationships between the genomes that comprise the pangenome and consequently we cannot determine when multiple alignments match to homologs in different genomes or paralagous copies dispersed across one or many genomes.
The linear growth in data scale can add algorithmic complexity when sequencing many genomes, and so it is understandable that experiments using the CHIC aligner only used up to 100 genomes at a time.
Because it does not model homologies between sequences, an unfolded pangenome cannot represent novel recombination between known sequences.
Finally, by relying on the linear reference to report alignments, CHIC gains the ability to hook into standard resequencing workflows, but it loses the ability to describe variation in sequence that is not contained in the reference.

\subsection{The variation graph}

The implication of these examples is that we need a data model for the reference that losslessly encodes sequence-level relationships between genomes, in effect representing a kind of all-versus-all alignment between the sequences in the pangenome.
If the data model is to allow recombination between known sequences (a key contributor to genomic diversity) and linear copy number variation (which occurs readily in genomes), then it could naturally be represented as a regular language, and thus could be represented using a graphical model like a nondeterministic finite automaton.

We can adjust the regular language model slightly so that it has properties similar to those of DNA.
Traditionally, NFAs are represented graphically with states (in our case, pangenomic coordinates) as nodes connected by edges labeled by characters (e.g. DNA bases) in the alphabet of strings that the language recognizes.
In DNA the essential component is the DNA base, which is represented as a character in a string.
Strings are thus often regarded as an essential object in bioinformatics, and graphical models with nodes labeled by sequences and edges representing allowed transitions between them are a straightforward generalization of the linear string and have often been employed in bioinformatics.
As maintaining identifiable positions requires the storage of auxiliary information such as their identity, compression can be achieved by allowing the labels on the nodes to have more than a single character on them.
This also allows the model to represent de Bruijn graphs, whose nodes are labeled with $k$mer strings.
Because DNA is double-stranded, any such a language implies a reverse complement language which recognizes the reverse complement of any sequence in the first.
Formalizing this by allowing edges to transition between different strands of the graph allows the model to directly represent sequence inversions.

If we combine these adjustments, we arrive at a kind of regular language model that resembles DNA and allows the representation of collections of related sequences in a compressed manner by allowing us to represent homologous sequences and all kinds of natural polymorphism\footnote{Note that the model does not encode } between them.
Formally, this structure is often referred to as a \emph{bidirectional DNA sequence graph}, indicating that the graph is sequence-centric, directed, stranded, and allows transitions between strands.
\emph{Sequence graph}, then implies a simpler concept in which the graph is meant to model sequences but only one strand is considered.
Assembly and multiple sequence alignment methods have employed sequence graphs of both types since the earliest computational analyses of biosequences, and it is sensible that we might employ them to represent collections of genomes.

The conceptual basis of the work I present here is the extension of the bidirectional DNA sequence graph with \emph{paths} that may be used to describe sequences as walks through the graph.
In this way, the panel of reference genomes or sequences used to construct the graph may be formally related to the graph itself, and the relationships between them made evident in the structure of the graph.
Existing knowledge expressed in terms of known sequences may thus be projected into the graphical pangenome model.
Similarly, entities within the graph may be surjected out into the space of a given reference.
These properties ensure the pangenome fully encompases existing reference technologies.
In addition, maintaining the sequences in the space of the graph makes the graph lossless, and it fully represents the input sequences without additional information.
Perhaps most importantly, this feature resolves the exponential decay in mutual information which limits the applicability of Markovian models like the sequence graph to modeling natural sequences, which tend to exhibit power-law decay in mutual information \cite{lin2017critical}.
I term this combination of a bidirectional sequence graph and paths a \emph{variation graph}, as it represents sequences and the variation between them.
In the subsequent chapter I will formalize the variation graph model and important auxiliary data structures that enable its modification and use in resequencing.

\section{Graphical techniques in sequence analysis}

The coherent representation of collections of sequences supported by sequence graphs has formed the core data model of many genome analysis patterns.
The alignment algorithms described in \ref{sec:sequence_alignment} can be modeled as a matrix, but similarly may be described as graphs, although the nodes in these graphs correspond to matrix cells and thus alignment states rather than characters or sequences.
Similarly, hidden markov models (HMMs) have a long history of use in bioinformatics \cite{durbin1998biological}, and the Markovian properties of these models bear similarities to the bidirectional sequence graph model.
Here, I will focus on those methods that are most-closely related to variation graphs, and upon which my work draws most heavily.
These include techniques for generating and encoding multiple sequence alignments, genome assembly graphs, and the related gene model graphs used in RNA sequence analysis.
The variant representation developed during the 1000GP, reified in VCF, encodes a directed acyclic sequence graph.
My work draws on the idea that a population of genomes can be represented in such a structure, which is enabled by the semantics of VCF.
The {\tt vg} data model can be understood as an extension of this sequence DAG so that it becomes equivalent with assembly graphs without losing the ability to represent phase, haplotype, and other sequence based genomic entities within it.

\subsection{(Multiple) sequence alignment}
\label{sec:MSA}

Optimal multiple sequence alignment generalizes the problem of pairwise sequence alignment from a 2D matrix to an $N$-dimensional lattice, where the optimal mutual alignment of $N$ sequences of average length $L$ can be determined in $O(L^{N})$ time \cite{carrillo1988multiple}.
In the early days of sequencing, when only a handful of sequences might be considered in one analysis, such costs could nearly be accepted, even if they limited the number of sequences in the MSA to only 3.
By pruning regions of the lattice in which no optimal alignments could occur, the authors of the tool ``MSA'' increased the number of sequences which could be optimally aligned into an MSA to 6 \cite{lipman1989tool}.
As the number of sequences to be compared increased, algorithms allowing MSAs to be generated from less than 10 sequences
In contrast to the optimal alignment approach, CLUSTAL builds a guide tree based on alignment of all sequences to all others in $O(NL^{2})$ time, and then generates the MSA progressively using the guide tree in $O(L^{2}\log N)$ time, resulting in a polynomial time algorithm capable of generating MSAs for hundreds of sequences contemporary computers \cite{higgins1988clustal}.
In this form of progressive alignment, the MSA is extended with alignments between closest pairs of sequences within the guide tree, then extended to produce the mutually gapped MSA matrix linking the two branches that have been aligned together with the pair of most-closely related sequences.
The progressive approach is fundamentally greedy, and susceptible to errors that propagate along the guide tree, although such errors can be mitigated by structuring the alignment using biological priors \cite{thompson1994clustal}.
Further improvements to the quality of the MSA can be gained by guiding the progressive alignment with a limited kind of global information about the relationships of all the sequences, as in T-COFFEE \cite{notredame2000t}, but this popular method exhibits a worst-case computational complexity of $(O)N^{3}L^{2}$.

The progressive alignment in MSA algorithms like CLUSTAL may be represented graphically.
In the late 1980s Eugene Myers and Webb Miller developed algorithms to optimally align sequences to sequence graphs, and sequence graphs (in the form of regular expressions) to each other in $O(MN)$ time (where $M$ and $N$ are the sequence length of the graphs) \cite{myers1989approximate}.
These algorithms provide a solution to the generic problem of aligning bidirectional sequence graphs to each other using DP, but to my knowledge were never implemented in publicly available software for sequence analysis\footnote{Myers was unable to provide related source code on request.}.

Christopher Lee later provided the first implementation of an MSA algorithm based on sequence to graph alignment \cite{lee2002POA}.
Apparently unaware of the work of Myers and Miller, he instead built on the concept of ``consistent equivalence relations'' that DIALIGN used to represent the MSA \cite{morgenstern1996multiple}.
Where DIALIGN's authors appear to consider the MSA in the space of the $N$-dimensional lattice, Lee encoded the equivalence relations in a partial order graph, which is often referred to as a directed acyclic graph (DAG).
In this DAG, characters label nodes and edges label observed linkages between them in sequences embedded in the MSA.
Lee demonstrated that a straightforward generalization of the recurrence relations used in Smith-Waterman-Gotoh would allow the alignment of sequences of length $N$ to the DAG of sequence length $M$ in approximately $O(MN)$ time.
To determine the score at a given position, POA considers matches and deletions relative to all the characters that immediately preceed the current one in the partial order.
Later, POA was extended to allow the direct alignment of pairs of MSAs using partial order to partial order alignment (PO-POA) \cite{grasso2004combining}.
Like CLUSTAL-W and other progressive methods, POA would build its MSA using pairwise alignments across a guide tree.
But, rather than aligning the last pair of sequences, PO-POA alignment would be used to align the MSAs from each branch together.
The improved information made available to the alignment inference resolved problems with order dependence, yielding a method with nearly the same performance\footnote{Here goodness is quantified using a sum of pairs score (SPS) metric representing the goodness of the alignment.} as T-COFFEE across a range of problems.
Similar sets of sequences will tend to compress when aligned with POA, and PO-POA complexity is in the order of the length of the pairs of aligned sequences.
It follows that as the MSA algorithm proceeds over a neighbor joining guide tree, the overall complexity of the resulting MSA algorithm has a lower bound of approximately $O(N\log{N})$.
Grasso and Lee confirm this bound is followed in practice by demonstrating that their implementation scales subquadratically in the size of its input.

Given that it could be three orders of magnitude faster that other popular when computing the MSA of only a few hundred sequences, it may be surprising that POA did not become the dominant MSA method.
A cubic algorithm, T-COFFEE, which Grasso and Lee showed to slightly but consistently outperform POA, has been cited ten times more.
As I discussed earlier in the opening of this chapter, the importance of obtaining the optimal alignment of a limited number of expensive sequences meant that even three orders of magnitude faster performance offered by POA did not matter for most users.
The low rate of use meant that the same alignment concept was ``rediscovered'' by participants in the 1000GP who became interested in the problem of aligning sequences to the sequence DAGs represented in VCF\footnote{Sequence to graph DAG alignment was one of Deniz Kural's main PhD projects. I worked with him in Gabor Marth's laboratory, and we applied his implementation of POA to generate accurate genotype likelihoods for indels and complex variation in the final phase of the 1000GP. We learned of the prior work later, when a reviewer pointed out that the algorithm was roughly equivalent to POA.}.

Lee's POA model provides a simple pattern for thinking about pangenomes, and its simplicity (it contains only node/characters and edges) in some ways differentiates it from others that have seen popularity in assembly, which I will describe in detail in the following section.
Its structure maps into finite automata, regular expressions, HMMs, and other models while erasing the conceptual barrier between sequences and sequence graphs.
However, this mapping is incomplete as POA MSAs are linear objects which cannot capture many natural kinds of genetic variation such as repeats rearrangements without duplication of the rearranged sequences.

Several methods have extended the MSA concept to unordered graphs, including the Threaded Blockset Aligner (TBA) \cite{blanchette2004aligning}, which models the MSA graph as a set of partially ordered MSAs connected in an unordered manner at a larger scale, and the A-Bruijn Aligner (ABA), which models the MSA using a de Bruijn graph and represents a solution to problems in MSA using techniques that later become important to short read assembly \cite{raphael2004novel}.
Variation graphs generalize these models without the limitations of order (as in POA and TBA) or $k$mer based graph structures (as in ABA), but in generality they lose the very features that help the performance of these methods, a fact that has likely inhibited their development until the present.

\subsection{Assembly graphs}

The problem of assembling large genomes is not dissimilar from that of multiple sequence alignment.
MSA algorithms tend to be applied to the alignment of a single coherent genomic locus.
Their input sequences might be expected to have approximately the same length, and maintain synteny between them.
Whole genome shotgun assembly methods cannot rely on such assumptions, as reads of a large genome rarely overlap, and both strands of DNA will be sampled, yielding ambiguity about relative orientation.
Thus the graphical models used in assembly must maintain bidirectional structure, and are most-generally bidirectional sequence graphs as previously defined.
OLC assembly methods in some sense began their evolution with the set of compromises that MSA algorithms ended theirs, with thrifty heuristic methods being used to establish the overlaps between all pairs of sequence reads from a given genome, and the resulting overlap set subsequently represented in a graphical fashion.
As we saw, early assemblers would enforce a linear ordering, in a way forcing a form of overlap DAG.
This approach makes sense only for very short genomes with minimal repeats and reads long enough to be uniquely localized in the reference genome.
As overlap alignments can occur between any pair of sequence strands, assembly methods must implement a full biridectional graph model if they hope to represent ambiguity in the overlap set, and so by default must be more generic than linear MSA models.
Earlier in section \ref{sec:genome_assembly} I gave a historical outline of the development of these methods in response to changes in available sequencing technology.
In this section I provide deeper technical detail to describe these methods and illustrate their relationship to the variation graph model.

\subsubsection{Overlap graphs}

One interesting feature of overlap graphs graphs is that their efficient construction tends to yield a representation in which sequences attached to nodes partially overlap with the nodes in their neighborhood in the graph.
This follows from the fact that most of the graph induction algorithms require a kind of pairwise alignmen, and so retain relationships between different sequences in a pairwise rather than compacted N-wise form.
In the case of DBGs and the FM-index based string graph assemblers, an iterative overlap-wise sequence comparison where unitigs (unbranching sequences in the graph) are inferred from the compressed sequence graph yields a similar overlap graph output.
It is thus standard to think of the edges in an assembly graph as representing the overlaps between the nodes they connect and their length.
This feature represents an incomplete compression of the information in the graph, which typically is not important to the use of these methods as they are judged by the quality of the set of contiguous sequences (contigs) they output rather than their raw assembly graph.
These methods will traverse linear portions of the graph to generate contigs, after pruning or ignoring edges, which the uncompressed overlap representation does not inhibit.
If we wish to use similar graphs as reference systems, where singular positions and sequence representations are desireable features, the overlap graph model is more complex to use than a ``bluntified'' representation in which the overlaps are non-ambiguously reduced into the nodes in the graph and its linkage topology.

In their most correct form, overlaps are themselves alignments, and as we have seen in section \ref{sec:MSA} have a natural encoding in graph form.
This means that assembly data models often encode multiple kinds of graph structures in a layered way, a fact which is reflected in the complexity and redundancy of the GFA version 2 specification\footnote{\url{https://github.com/GFA-spec/GFA-spec/blob/master/GFA2.md}}.
In general this overlap representation makes it difficult to work directly with such graphs using the algorithms we will introduce, but these graphs remain relevant to this work as they may be reduced into a ``blunt-ended'' bidirectional graph.

% mention miniasm and friends, bit of grounding in the present

Overlap assembly graphs, like MSA graphs, may be converted into variation graphs and used as a basis for resequencing.
It is also possible to directly induce a bidirectional string graph from a set of pairwise alignments, sidestepping this issue.
Later, I will present my design and implementation of an external memory algorithm that transforms the alignments into a graph through a transitive closure of the equivalencies implied by the alignments.
This directly transforms the alignments which might be used by an assembler into a blunt-ended assembly graph.

\subsubsection{De Bruijn graphs}

De Bruijn graphs \cite{de1946combinatorial} are graphs in which a set of $k$mers are taken as the nodes of the graph, and edges are added for each pair of $k$mers $k_1 \rightarrow k_2$ in which the last $k-1$ bases of $k_1$ are the same as the first $k-1$ bases of $k_2$.
As discussed previously in section \ref{sec:genome_assembly}, this model simplifies the overlap graph structure, allowing efficient calculation and representation of the graph.
For instance, $k$mer lengths may be chosen so that they fit inside a machine word, allowing bitwise operations and integer math rather than string comparison to be used to infer the graph structure directly implied by the $k$mers themselves.

In second generation sequencing, where per base error rates are low and read lengths are short, little is lost by breaking the read set into $k$mers of 1/3 or 1/5th the length of the original reads, and so this model has been readily applied to cheap short read sequence data since its introduction to genomics in the mid 1990s and early 2000s \cite{idury1995new,pevzner2001eulerian}.
The method was first implemented for short reads in Velvet \cite{zerbino2008velvet}, which used a straightforward but memory-costly hash table strategy to encode the DBG.
Zam Iqbal then extended this model to support various kinds of pangenomic analysis based on DBGs by labeling each $k$mer with ``colors'' representing read counts from different samples \cite{iqbal2012novo}, while later versions of the Cortex assembler have extended it to fully encode long reads or contigs relative to the DBG \cite{turner2018integrating}.
Similarly, improvements in performance have been yielded by linking read pairs in the DBG model \cite{bankevich2012spades}.

It was be shown that the DBG is encoded in a FM-index of a read set \cite{bowe2012succinct}, and this \emph{succinct} DBG model underpins some of the most-scalable assembly methods currently available \cite{li2015megahit}, while efficient encodings using other models such as bloom filter schemes has enabled the most memory-efficient DBG methods \cite{chikhi2012space}.
In the compacted DBG generalization non-bifurcating regions of the graph might be merged into a single node with label length $>k$.
The compacted DBG is now often a typical output for DBG assemblers \cite{chikhi2016compacting,minkin2016twopaco}.
The compressed nodes and their overlaps with their neighbors form a set of unitigs that are often taken as the most-raw kind of assembly output, to which popular assemblers like SPAdes and Minia3 apply a kind of graph simplification finishing step to infer longer contigs from them \cite{bankevich2012spades}.
As with any overlap graph, DBGs must be made into blunt-ended sequence graphs before they can be utilized by variation graph based algorithms.
The basic method for doing so is simpler than for generic overlap graphs, as overlaps in DBGs must be exact string matches.
In my work I have found this an important feature, as in addition to being generated by efficient methods, it ensures that DBGs are universally reusable as variation graphs.

\subsubsection{String graphs}

The string graph is a formalism that describes the full information represented by a shotgun sequencing experiment and an all-against-all alignment between its reads represented as set of approximate overlaps between the read ends and containments between them \cite{myers1995toward,myers2005}.
Myers argued that the then-current paradigm of assembly, which attempted to generate the shortest common superstring (SCS) incorporating all of the $N$ input sequence reads, failed to reconstruct the genome correctly in the context of repeats in the genome that are longer than the average read length $\overline{L}$.
He then posited what he called the ``chunk graph'' (later, string graph) as a graphical model of the overlap set, showing that the correct consensus sequence would by definition exist as a walk through the graph.
In this graph nodes represent sequence reads and directed edges represented observed approximate overlaps between them.
Repeat units in the genome that are longer than $\overline{L}$ will collapse in this graph, provided errors in the reads can be corrected so such repeats become fully identical.
The string graph compresses its input.
The edge complexity of the graph may be reduced from the number of read overlaps by a factor proportional to the average coverage by removing transitive edges.
Nodes may similarly be compressed where runs of nodes then have in and out degree one.

This idea was introduced at the same time as de Bruijn graphs, with Myers' work on string graphs and the first description of a genome assembly algorithm using de Bruijn graphs both presented at the same workshop in 1995 \cite{myers1995toward,idury1995new}.
Myers' 2005 formalization of the string graph \cite{myers2005} responds particularly to de Bruijn models, and he points out that generating $k$mers from the reads as the basis for the graph means that the resulting graph is not ``read coherent'', or in other words does not accurately represent the information in the read set.
Subsequent work has shown that the boundaries between the two models are not so well-defined.
As a specialization of the overlap string graph, the de Bruijn model may be applied to certain complex subsets of a string graph, creating a kind of hybrid assembler where the high-performance $k$mer model is used to resolve the most-difficult components, while the generic overlap model is used elsewhere \cite{huang2016integration}.
Similarly, the high cost of error and graphical complexity suffered by the string graph encourage the use of $k$mer based read correction models, which could be seen as filtering the reads using a DBG model.

String graphs are often described as ``lossless'' representations of the input read set and the alignments between them \cite{li2012exploring}.
Neither in practice nor in Myers' formalizations nor its implementations like the Celera assembler (CABOG) \cite{miller2008aggressive} is this strictly true.
In the model, overlaps are assumed to be $\epsilon-$correctable at approximately the raw sequencing error rate.
In practice, this filtering can result in a loss of input sequence from the string graph.
String graphs can consume very large amounts of memory when fully constructed without filtering from a read set \cite{li2016minimap,koren2017canu}.
Input filtering, mostly to reduce repeat content, is used to mitigate this issue.
If not aggressively corrected, repeats tend to generate ultra-dense graph regions that are known as ``hairballs'' which can increase the complexity of assembly by orders of magnitude.

To deal with repetitive sequences in the genome, one solution is to mask out repetitive $k$mer, minimizer, or alignment seeds used in generating the overlap set, as in CABOG, FALCON, and miniasm \cite{miller2008aggressive,chin2016phased,li2016minimap}.
Recently, alternative probabilistic seed filtering based on the \emph{tf-idf} (term frequency, inverse document frequency) metric has shown promise at retaining information about repeats and allowing their separation rather than excision from the assembly string graph \cite{koren2017canu}.
%In principle these masked regions can be replaced by simplified models of particular repeat classes, but this appears difficult to achieve and has not been a priority as historically assembly graphs have not been directly used in resequencing.

String graph methods related to the Celera assembler (FALCON, Canu) implement error correction steps before overlap inference, as this reduces memory requirements during assembly.
Similarly, popular methods that generate assemblies via a string graph induction step from Illumina sequencing data (SGA, fermi, and fermikit) also apply an error correction processes before the generation of the string graph \cite{simpson2012efficient,simpson2014exploring,li2015fermikit}, which helps to reduce the graph complexity and improve contiguity of the resulting assembly.

Heterozygosity in a string graph should result in a bubble, or graph component connected to the rest of the graph via single sources and sinks.
Like de Bruijn graphs, string graphs have been used to support variant calling, for instance finding heterozygotes in the assembly graph of a single individual \cite{li2012exploring}.

With minimap and miniasm Heng Li demonstrated that it is also possible to build an approximate overlap graph using an efficient all versus all alignment \cite{li2016minimap}.
The application of an efficient, if greedy contig generation step results in an assembly in which the error rate in the contigs approaches that of the input reads.
Instead of correcting before generating overlaps, the consensus step can be used to improve the base level accuracy of the assembly after it has been generated, but tools like racon that implement this approach do not do so on the assembly graph \cite{vaser2017fast}, and rather work on the level of individual contigs.
Li \cite{li2016minimap} also proposed to mix and match different assembly components (for instance using DALIGNER \cite{myers2014efficient} miniasm for the overlap step, or swapping quiver for nanopolish for consensus generation) by establishing a set of standard data types including the pairwise alignment format (PAF) and the graphical fragment assembly (GFA).
These encode the results of the overlap step and a graphical model for the assembly at any state of its progression.
GFA is used in a wide number of methods, but as of August 2018 it appears that very few tools both read and write GFA, and much of the assembly improvement steps are implemented on contigs (encoded in FASTQ or FASTA format) rather than the string graph itself.

% recap on string graph concept
% implementations
% it's an overlap graph yep

%The string graph can be understood as the graph implied by the mutual alignment of all the sequence fragments input into the assembly.
%Each position in two reads that aligns as a match becomes a single node in the graph.

\subsubsection{RNA splicing graphs}

% could be developed by an assembly algorithm directly
% in model organisms we can use curated gene models to build
% not assembly graph as derived from gene model
% which algorithms have used these methods?
% for de novo assembly \cite{grabherr2011full}
% \cite{schulz2012oases}

\subsection{Variation aware alignment}

% reference + variant alleles = sequence DAG
% ``if we could'' describe SVs it might be not DAG like, but the reference would still be its backbone

%\subsubsection{The variant call format}
% what is the variant call format?

\subsubsection{1000GP indel genotyping}

% quick summary with citation of supplement describing it 

\subsubsection{GraphTyper}

% uses VCF to construct a graph to which reads mapped into a given reference window
% like 1000GP indel genotyper

\subsubsection{Bwbble}

% a way of encoding the VCF data into a sequence index

\subsubsection{Population reference graphs}

% another fun way, now using DBGs and possibly MSA sequence alignments rather than VCF
% but definitely into the reference, focused on a small area rather than the whole problem

\subsubsection{Gramtools and the vBWT}

% write the DAG into the index provided a number of structural limitations
% problems with nesting (yes could be corrected) and cost of large alphabet WT based FM-index

\subsubsection{Seven Bridges Genomics' graph tool}

% whole genome alignment to particular kinds of graphs
% local alignment method (based on path enumeration) limits variant density that can be allowed
% results in highly variable runtime given graph structure
% DAG only

\section{Overview of this work}

% nothing out there meets all the variation graph model needs
% boom
 
