%!TEX root = ../thesis.tex
%*******************************************************************************
%****************************** Second Chapter *********************************
%*******************************************************************************

\chapter{Variation graphs}

\ifpdf
    \graphicspath{{Chapter2/Figs/Raster/}{Chapter2/Figs/PDF/}{Chapter2/Figs/}}
\else
    \graphicspath{{Chapter2/Figs/Vector/}{Chapter2/Figs/}}
\fi

Graphical models used to compress collections of genomic sequence data are unable to losslessly reproduce their own input.
This is plainly seen by considering the an alignment graph where two homologous sequences have an internal homology between them.
Such a graph will represent four possible input sequences as walks through its nodes and edges, and without additional labeling it will be impossible to obtain its input from the graph itself \cite{kehr2014genome}.

While the input sequences exist in the set of sequences that can be generated by walks through the graph, as the complexity of the graph grows the number of paths will increase exponentially.
If annotated with transition weights between nodes representing the frequency which different edges are taken by sequences in the input, then the graph can be used as an HMM to simulate new sequences.
While these will have local similarity to the graph's preimage, they are unlikely to provide a good model for the input sequences.
Sequences generated from an HMM will exhibit an exponential decay in mutual information between their symbols, while natural sequences tend to exhibit power-law decay in mutual information \cite{lin2017critical}.
This observation suggests that no pure markovian model will allow us to faithfully represent its input, and in turn that the only way to be sure that we model the input sequence space is to maintain a record of it in the context of the graph.
The compression provided by the graph is still relevant, as we can exploit the repetitive nature of natural sequences to obtain compression of this set, but using lossless techniques.

\emph{Variation graphs} (VG)\footnote{I will refer to variation graph as VG, and to the software implementation of the VG model {\tt vg}}, previously introduced in section \ref{sec:the_variation_graph}, combine a bidirectional sequence graphs with paths that model their preimage as walks through the positional space of the graph.
This makes them representationally equivalent to both graphical models and linear sequence models, and allows them to be used to model the relationships between collection of sequences, including all variation contained therein.
The encapsulation of these two divergent ways of modeling about bioinformatic data systems allows them to bridge traditionally isolated analysis modalities.
We can define these graphs by their construction from a set of pairwise alignments between sequences, as in the alignment, Enredo, and Cactus graph models, but if we want to relate new sequences to them then we would need to rebuild the entire graph to include these sequences.
By developing a model for an alignment against the variation graph we can express the relationship between new sequences and the graph without the cost of embedding the new sequence in the graph.
This allows us to follow the same kinds of analysis patterns used in resequencing, where a common reference system is used to compute a form of joint assembly between (potentially) many samples in an out of core manner.
Further, in such a scheme the reference system can remain relatively stable, which has relevance for practical use.

In this chapter, I will articulate the variation graph model and lay out the algorithms and data structures that enable its use as a reference system in pangenomic resequencing.
First I will provide formulations for the graph, its paths, edits, alignments, and genotypes define within it.
Then I will present algorithms that induce the variation graph from different data models introduced in the previous chapter.
To clarify the system's practical basis, I explain the serialization techniques used to exchange variation data.
A main focus of my work has been the efficient alignment of sequences to variation graphs.
This requires index structures to store the graph and enable queries of its topology, sequence, and path spaces, and algorithms to drive the inference of optimal alignments to the graph.
Understanding variation graphs requires techniques to visualize them, and I will present various approaches, each with particular advantages and drawbacks.
Working with variation graph references necessitates a number of mutating operations to the graph, including augmentation, sorting, pruning, and bubble simplification.
Finally, I will discuss how variation graphs can provide normalized basis spaces for the analysis of pangenomes, such as through various decompositions of alignment sets and the graph including coverage maps, ultrabubble decomposition, and haplotype matching.

\section{An extensible graphical model of many sequences}

We define a variation graph to be a graph with embedded paths $G = (N, E, P)$ comprising a set of \emph{nodes} $N = n_1 \ldots n_M$, a set of \emph{edges} $E = e_1 \ldots e_L$, and a set of \emph{paths} $P = p_1 \ldots p_Q$, each of which describes the embedding of a sequence into the graph.
At its heart, this model is simply a bidirectional sequence graph with a recorded set of walks through the graph topology.
By generalizing these paths to encode edits against the graph, we provide a mechanism to describe relations between the graph and other sequences.
Augmenting the path with additional information important to sequence analysis allows us to construct an \emph{alignment}, which forms the core data type in resequencing.
Collections of pairs of paths covering the space of two graphs describe a graph to graph alignment, or \emph{translation} which can be generated when the graph is edited to allow for the projection of coordinates and sequences in one graph into the space fo the other.
A limited form of this translation is a \emph{genotype}, which maps the implied bubble formed across multiple copies of a homologus locus in one genome into the space of the graph.
Collections of genotypes are the primary output of resequencing.
Phasing algorithms extend genotypes into longer phased haplotypes, which have a natural representation as paths through the graph.
These data models thus provide a sufficient informational basis for resequencing against variation graphs.

\subsection{The bidirectional sequence graph}

Each node $n_i$ represents a sequence $seq(n_i)$ that is built from an alphabet ${ \cal A } = \{ {\tt A, C, G, T, N} \}$. Nodes may be traversed in either the forward or reverse direction, with the sequence being reverse-complemented in the reverse direction.
We write $\overline{n_i}$ for the reverse-complement of node $n_i$, so that $seq(n_i) = revcomp(seq(\overline{n_i}))$.
Note that $n_i = \overline{\overline{n}}$. For convenience, we refer to both $n_i$ and $\overline{n_i}$ as ``nodes''.

Edges represent adjacencies between the sequences of the nodes they connect.
Thus, the graph implicitly encodes longer sequences as the concatenation of node sequences along walks through the graph.
Edges can be identified with the ordered pairs of oriented nodes that they link, so we can write $e_{ij} = (n_i,n_j)$.
Edges also can be traversed in either the forward or the reverse direction, with the reverse traversal defined as $\overline{e_{ij}} = (\overline{n_j},\overline{n_i})$.
Note that graphs in vg can contain ordinary cycles (in which $n_i$ is reachable from $n_i$), reversing cycles (in which $n_i$ is reachable from $\overline{n_i}$), and non-cyclic instances of reversal (in which both $n_i$ and $\overline{n_i}$ are reachable from $n_j$).

\subsection{Paths with edits}

We implement paths as an edit string with respect to the concatenation of node subsequences along a directed walk through the graph.
We do not require the alignment described by the edit string to start at the beginning of the sequence of the initial node, nor to terminate at the end of the sequence of the terminal node.
To allow the path model to support differences from the graph, each path is composed of a series of node mappings $p_i = m_1 \ldots m_{|p_i|}$ which are semantically identical to the alignment format used by standard aligners.
Each mapping $m_i = ( b_i, \Delta_i )$ has a starting position encoded as a node and offset in the graph $b_i = ( n_j, o_i )$ and a series of edits $\Delta_i = \delta_1 \ldots \delta_{|m_i|}$.
Edits $\delta_i = ( f_i, t_i, r_i )$ represent a length $f_i$ in the graph node $n_j$ (a ``from length'' in the reference), a length $t_i$ in the sequence the path encodes (a ``to length'' in the query), and encode an additional sequence $r_i$ that would replace the sequence at the given position in the reference in order to transform it into the query.
In the case of exact matches, we allow the replacement sequence $r_i$ to be empty.
They describe a function that transforms \emph{from} the graph \emph{to} the sequence.
Edits allow for rich encoding of alleles, but in practice alignments are rendered in terms of matches, mismatches, and indels.
We encode matches when $f_i = t_i \land r_i = \emptyset$, single mismatches when $f_i = t_i = 1 \land r_i \neq \emptyset$, deletions when $f_i > 0 \land t_i < 0 \land r_i = \emptyset$, and insertions when $f_i = 0 \land t_i > 0 \land r_i \neq \emptyset$.
As paths are described by a series of mappings with independent positions, they can represent all kinds of structural variation that can be represented in the bidirectional graph itself, including translocation, inversion, and copy number variation.
When mapping positions are always at the start of a node, the edit set for the path contains only matches, and the edges traversed by the path are all present in the graph, we say that the path is \emph{embedded}.
The paths from which we construct the variation graph are fully embedded, and in practice paths that contain differences occur only in the alignment of new sequences into the graph.

\subsection{Alignments}

Additional auxiliary information is important when analyzing collections of DNA sequencing data sets.
Each read has a name, and an identity related to a particular sequencing experiment.
It may be related to a particular genomic sample or individual.
DNA sequence reads themselves result from a previous set of analyses run on raw observations derived from DNA, perhaps fluorescence or current traces or images.
The process of collapsing this raw information into the sequence read yields a confidence in addition to a base call.
These are recorded in a quality string in FASTQ.
The need to collect this information has resulted in the development of the SAM/BAM sequence alignment format, which provides a standard for linking the called bases (sequence), quality information, read name, features of the alignment against a reference genome and additional optional freeform annotations.

I follow this same model in developing an alignment format for read alignments to the graph.
An aligned set of sequences $Q$, $A = a_1 \ldots a_{|Q|}$, represents a sequencing experiment.
Each aligned read connects a sequence, an (optional) quality string, a path through the graph, and an optional set of $D_i$ annotations: $a_i = (s_i, q_i, p_i, k_1\ldots k_{D_i})$.
In principle the read sequence can be reconstructed from the path, but retaining the sequence information makes the alignment object lossless with respect to the input FASTQ and provides redundancy which can help in data processing.
Alignments may thus be reprocessed without reference to the graph to which they were aligned.
The serialization of this alignment format is called GAM, for Graphical Alignment/Map, in analogy to SAM (Sequence Alignment/Map format).

At their heart alignments redundantly link a sequence with a path through the graph.
They form the basis for operations on the graph, as new sequences mapped into the graph may be used to extend the graph itself by editing the graph to include the sequences represented by the alignments.
Just as alignments describe a function to edit the graph, they can describe the relationship between the sequence space of two graphs.

\subsection{Translations}
\label{sec:translation}

A generalization of the alignment is the translation set $\Phi = \phi_1 \ldots \phi_{|\Phi|}$, which relates paths in different graphs to describe the alignment between them.
A translation $\phi = (p_f, p_t)$ defines the projection between two paths which may arise in the context of two graphs $G_f$ and $G_t$.
In this use each $p_f$ corresponds to a path relative to $G_f$ (conventionally the base or reference graph), and each $p_t$ to some path in $G_t$.
However, both paths in each pair could be relative to the same graph, in which case translations allow us to descibe collections of allelic differences to the base graph.

If each node in both graphs is represened in some graph translation in $\Phi$ then it provides an isomorphic relationship between the coordinates and sequences in both graphs.
Provided each $\Phi$ encodes an isomorphism, then we can layer a series of $\Phi_i$ together to provide a coherent coordinate space across any number of updates to a given graph.
Consider a function pattern $translate$, which allows the projection of paths relative to $G_f$ through translations $\Phi$ to yield paths in $G_t$: $translate(p_i, \Phi) \Rightarrow p_j \in G_t$, and similarly allows the transformation of a base graph into a target graph: $translate(G_f, \Phi) \Rightarrow G_t$.
If we have a series of $(G_i, \Phi_1) \ldots (G_\sigma, \Phi_\sigma)$, where $translate(G_i, \Phi_i) \Rightarrow G_{i+1}$ and thus each $\Phi_i$ describes an isomorphism between $G_i$ and $G_{i+1}$, then we can generate a graph translation $\Phi_\Delta$ providing $translate(G_1, \Phi_\Delta) \Rightarrow G_\sigma$.
We build this graph translation with the function $layer(\Phi_\alpha, \Phi_\beta) \Rightarrow \Phi_{\alpha \cup \beta}$ by rewriting each path translation $\phi_i \in \Phi_\alpha$ so that its $p_t$ refers to $G_\beta$.
We do so by projecting the $p_t$ through $\Phi_\beta$, and finally adding any $\phi_j \in \Phi_\beta$ for which $p_f = \emptyset$, as these represent insertions of new sequence in $G_\beta$ relative to $G_\alpha$.

\subsection{Genotypes}
\label{sec:genotypes}

As path to path relationships can provide descriptions of allelic diversity, they form the basis for a graph-relative genotype encoding.
To represent the exact genotype of a particular sample with ploidy $\nu$ at a given locus $\iota$ we can simply collect the multiset of alleles $\pi_\iota = ( p_1 \ldots p_\nu)$.
We could alternatively build a probabilistic model $\varpi$ of an unphased genotype by using a set of $\mu$ alleles $\{ p_1 \ldots p_\mu\}$.
To do so we associate likelihoods $\gamma_\xi$ for each possible genotype $\pi_{\iota_\xi}$ that could be sampled from the alleles such that $\varpi = \gamma_1 \ldots \gamma_{\mu! \over{\nu!(\mu-\nu)!} }$.
In practice we develop our $\gamma_\xi$ out of quality information from the reads and a sampling model related to $\nu$ \cite{garrison2012haplotype,li2011stats}.
Given this definition of a genomic locus it is clear that existing genotyping models can be applied to drive genotyping using read sets aligned to the graph, and the output of the genotyper is defined fully in the space of the graph.

\subsection{Extending the graph}
\label{sec:extending}

Given an alignment $a_i$, we can edit the graph $G$ so that it includes the alignment and the sequence it represents as an embedded path, $augment(G, a_i) \Rightarrow (G', \Phi)$, such that $translate(p_i, \Phi) \in G'$.
To update the path space of the graph we project all paths, including that of $a_i$, through the translation implied by the augmentation of the graph with $p_i$.
Any other $a_j$ whose path $p_j$ overlaps $p_i$ would no longer be valid, although it could be projected through the graph translation $\Phi$ as well to express it in the space of the new graph $G'$.
Updating the graph one alignment at a time is inefficient as we need to build and layer a new translation for each alignment.
It is simpler to edit the graph in a single step, taking a collection of alignments and including them in the graph, $edit(G, A) \Rightarrow (G', \Phi)$.

One way to accomplish this is to first take the set of unique mappings represented in the paths of $A$, $\Omega = \{ m_1 \ldots m_{|\Omega|}\}$, and for each $m_i$ edit $n_i$ at the breakpoints where any new variation would need to be added in.
Then, walking through each alignment we add in unique novel sequences and their linkages to the preexisting nodes or new breakpoints to the graph.
This process will disrupt the identifier space of the nodes and edges of the graph, but it naturally yields a translation that can be used as described in section \ref{sec:translation}.
Note that both alignments and genotypes are based on paths, so this mechanism can be used to extend the graph based on any sequence level differences that we observe either through alignment or variant calling.

\section{Variation graph construction}

We will use variation graphs as the core model for a number of essential processes in genome inference.
This model can represent many graphical sequence models used in genomics.
Each one necessitates conversion to project it into the variation graph model.
Here I describe the transformation of a number of graphical models into variation graphs, including MSAs, assembly graphs, and alignment graphs induced from pairwise alignments.
In some cases the conversion is direct, but in others it requires the addition of new labels to our model.
Variation graphs may also be built from first principles, provided a function that aligns a sequence into the graph and the editing operations described in \ref{sec:extending}.

\subsection{Progressive alignment}
%*1p 1h*
Assume a function \emph{align} that takes a sequence $Q$ and a variation graph $G$ and yields the path $p$ that is the best model of $Q$ in $G$, such that it maximizes a $\lambda$-parameterized measure $score_\lambda(p)$: or $p = \arg\max_{score_\lambda(p)} [\forall p \in G : seq(p) = Q]$.
Provided $score_\lambda$ scores exact matches above mismatches and gaps, the alignment given by $p$ will be a kind of minimal description of $Q$ in the basis of $G$.
Equivalently, it is a $\lambda$-minimal augmentation of $G$ required to include $Q$.

We can build a variation graph progressively from a set of sequences.
If we have a series of $k$ queries $q_1 \ldots q_k$, then we can build a progressive alignment by a series of edit and alignment operations applied to the variation graph.
First, take the empty graph $G_\emptyset$, to which any alignment will yield a path $p_1$ that has no mappings an which encodes the query sequence $q_1$ as a replacement sequence in the path.
We then edit the graph to add the sequence using $edit(G_\emptyset, p_1) \Rightarrow G_1$.
For each subsequent $q_j$ we obtain the next graph by finding the alignment $align(q_j, G_j) \Rightarrow p_j$ and editing the graph with it to yield the next graph $edit(G_j, p_j) \Rightarrow G_{j+1}$ until $j = k$ and we obtain our final graph $G_{\cup \forall q_i}$.
This simple approach is attractive as it allows the variation graphs to be built from whole sequences using only techniques that are native to the variation graph model itself.
I later describe an implementation of this process multiple sequence graph alignment, {\tt vg msga}.

\subsection{Using variants in VCF format}
%*1.5p 1h*
As discussed in section \ref{sec:seq_dag_vcf}, the VCF format that is popular in resequencing implies a sequence DAG.
We can consider how to build a variation graph using the core operation $edit$.
First, we build a variation graph from the reference genome $Q_\textbf{ref}$ : $G_\textbf{ref}$.
This graph contains one path $p_\textbf{ref}$ : $seq(p_\textbf{ref}) = Q_\textbf{ref}$.
As described in section \ref{sec:genotypes} each locus reported in VCF can be encoded as a set of paths $P_\textbf{vcf} = p_1 \ldots p_V$, each representing a different one of the $V$ alleles in the VCF.
We now edit the graph to embed these allele paths, $edit(G_\textbf{ref}, P_\textbf{vcf}) \Rightarrow G_\textbf{vcf}$.
Note that the graph has losslessly encoded the variant input and reference genome, and it is possible to regenerate the VCF file input by walking the positions of $p_\textbf{ref}$ and enumerating the overlapping paths as alleles in VCF format.

For efficiency, we have not implemented VCF to variation graph conversion with specifically this algorithm, but instead build up $G_\textbf{vcf}$ by walking along the reference genome $Q_\textbf{ref}$ and processing each locus independently.
This exploits the partially ordered property of the VCF to limit memory requirements when working with whole genome graphs.
For regions before, after, and between variant records at reference offsets $i$ and $j$ we add a node $n_\textbf{curr} : seq(n_\textbf{curr}) = substr(Q_\textbf{ref}, i, j)$, linking these by edges to those nodes ending at position $i$ of the reference and adding corresponding mappings for the reference path to $p_\textbf{ref}$.
At simple variant sites we add each of the alleles as a new node $n_{\textbf{var}_i}$, including an edge for each $e_{\textbf{curr} \prec \textbf{var}_i}$.
Here we also handle the reference allele differently in that we append a mapping $m_\textbf{ref} = ((n_\textbf{ref}, 0), \emptyset)$ to $p_\textbf{ref}$.
This approach seamlessly handles all kinds of common variation, including SNPs and indels, as well as more complex variation encoded as small haplotypes.

As long as the VCF records are ordered, this process allows for streaming conversion of the VCF format into a variation graph.
However, VCFs used to represent structural variation often do not describe a fully-ordered series of loci.
For instance, a large deletion may be described in one record, and followed by a number of records describing variation on the reference within the scope of the deletion.
In the graph, this results in the nesting of bubbles, and requires a deviation from a simple streaming algorithm in order to be handled.
Deletions must be recorded and linked into the downstream portion of the graph as it is later generated.
The reference allele for the deletion can only be defined once we have constructed the variation graph under the deletion.

VCFs may also encode phased haplotypes, which, like the reference, have a natural representation in the graph as paths.
Parsing these may require multiple passes over the VCF due to the memory requirements for storing large numbers of haplotypes uncompressed and cross-indexed to allow traversal in RAM.
To prevent $O(H|G_\textbf{vcf}|)$ growth of the required memory to store these, we implement efficient compression strategies on the haplotype set that exploit their repetitiveness.
There is no semantic requirement that the encoded haplotypes in VCF are valid, which introduces some complexity in the implementation of this method.
We must break haplotypes where they are found to be invalid in order to record them in VG format.
For instance, a phased VCF may report more than $\nu$ (expected ploidy) alleles for a given individual, such as when deletion and SNP variants overlap.
We expect these haplotype paths to be fully embedded in the graph an they can be represented as a walk through nodes.
Although haplotype sets are equivalent to large collections of paths, we term them \emph{threads} to indicate that they have a simpler representation than full paths.

\subsection{From gene models} % (GFF)

A reference-based RNA splicing graph is usually expressed as a set of alignments, perhaps as named intervals in BED or the General Feature Format (GFF).
As with the generic VCF generation algorithm, we can convert the transcripts to alignments $A$ relative to the graph.
Then we can then embed the transcript paths in the graph $edit(G_\textbf{ref}, A) \Rightarrow G_\textbf{splice}$.
Any transcript in our set is thus encoded by the graph, and can be matched to it directly with alignment.
We also allow for novel isoforms with local similarity to the known set.

\subsection{From multiple sequence alignments}
%*0.5p 0.5h*
A multiple sequence alignment in matrix form has a simple translation into a sequence DAG and thus a VG \cite{lee2002POA}.
Given a set of sequences $Q = q_1 \ldots q_\kappa$ their $\upsilon$-long mutual alignment may be described in a $\kappa \upsilon$ matrix $X$ designed to maximize $\sum_{i=1}^{\upsilon} \sum_{j=1}^{\kappa} \sum_{k=1}^{\kappa} \delta_{X_{ij}X_{ik}}$, where $\delta$ is the Kronecker delta.
The alphabet used to encode the matrix is the same as the input sequences with the addition of a special gap character $\Box \neq \Box$, and gaps thus do not contribute positively to the matrix score.
To build a variation graph $G_\textbf{msa}$ from the MSA we proceed from $i = 1 \to \upsilon$ through $X$.
For each unique character ${\cal B}$ in the query alphabet ${\cal A} \setminus \Box$ found in each row $i$, we create a node $n_{\cal B}$ in $G_\textbf{msa}$ and append a mapping to each path $p_i$ for which $n_{\cal B} \in q_i$.
We construct the edges of $G_\textbf{msa}$ by taking the distinct pairs of consecutive node traversals found in the path set $P_\textbf{msa}$ produced after the generation of the nodes in MSA traversal.
For each pair connecting nodes $n_i$ and $n_j$ we add an edge $e_{ij}$.
We can optionally compact series of nodes (which represent single characters) where no bifurcations occur, which is similar to the operation used to compact DBGs to yield unitig graphs.

Instead of a matrix, we can formulate the multiple alignment as an alignment graph (described in section \ref{sec:genome_alignment_graphs}).
By making this graph bidirectional, and thus equivalent in information content with the Enredo graph, we can see that it is equivalent to a variation graph.
Aligners that produce data formats of this type, such as Cactus \cite{Paten:2011fva}, can thus be used to produce VGs, so long as the relationship between the input sequences and the graph is recorded and can be converted into a path description.
I later use this technique to build a pangenomic reference system for a diverse set of yeast strains.

\subsection{From overlap assembly and deBruijn graphs}
%*2p 1.5h*

Overlap-based sequence graphs used in assembly, described in sections \ref{sec:overlap_graphs}, \ref{sec:de_bruijn_graphs}, and \ref{sec:string_graphs}, are nearly identical to variation graphs.
The critical difference between these models and the variation graph is that they attach a label to each edge describing the alignment between the pair of nodes which they connect.
Variation graphs do not support such a feature in their basic definition, as it is unimportant for any use of variation graphs besides temporarily representing overlap graphs.
This process is known as \emph{bluntification}, as the edges lose information and sequences given by traversing the graph are represented directly in the concatenation of node sequences.

If we reduce an overlap between a pair of nodes, it will render other overlaps on the same nodes incorrect.
Thus it is essential that the bluntification algorithm work by the reduction of sets of overlaps on edges which are transitively closed by connection to the same ends of each node, $net(e_{ij}) \Rightarrow \forall e_{i*} \in G \cup \forall e_{*j} \in G$, considering both strands of the graph when doing so.
For each net we apply a function $pinch(net(e_{ij})) \Rightarrow G_{\textbf{pinch}_{ij}}$, which reduces the overlaps between the nodes in the net into a blunt-edged variation graph.
We then link $G_{\textbf{pinch}_{ij}}$ back into the rest of the graph by connecting with the inbound links to each node involved in the net.
We may thus rewrite the input overlap graph as a variation graph.

In a de Bruijn graph or string graph such as SGA or Fermi2, overlaps are defined given only a length.
This simplifies the implementation of $pinch$, as no further computation is required to correctly determine the mutual alignment of overlapping sequences.
In contrast, overlaps in a generic string or overlap graph are correctly defined as alignments.
Resolving a single pairwise alignment into structures in the graph is trivial, but it becomes considerably more complex when many sequences map into a transitively closed set of overlaps.
These nets can then be resolved into an alignment graph by an algorithm similar to that given in section \ref{sec:from_pairwise_alignments}, but in practice, {\tt vg} implements bluntification by conversion of the input graph to a pinch graph library that was used to resolve the same problem in whole genome alignment \cite{paten2008enredo}.

% we have to add the overlap label to edges, but otherwise we cool
% then there is the pinch graph reduction
% probably can be explained pretty easily

\subsection{From pairwise alignments}
\label{sec:from_pairwise_alignments}

Although they are mathematically equivalent, the complexity of converting various graphical genome models to variation graphs can be considerable, and {\tt vg} still lacks an ideal solution to the problem.
A set of pairwise alignments imply a variation graph, however I know of no contained method that will generate the variation graph or lossless string graph from these alignments.
One apparent problem is the memory requirements of doing so.
The full string graph for a noisy or divergent sequence set is rarely possible to maintain in RAM.
This suggest that an external memory based approach may enable greater applicability of such an approach.
To explore this, I developed an algorithm to do so that operates in external memory, which I here present in detail.
It operates by conversion of the alignment set into an alignment graph and the subsequent use of this graph in the elaboration of the variation graph including paths representing the input sequences.
The resulting graph is a lossless representation of the input and the alignments between them.
To distinguish the approach from string graphs, which imply error correction, I call this variation graph induction model the \emph{squish graph}.

% seqwish algoritm
{\tt seqwish}\footnote{\url{https://github.com/ekg/seqwish}} implements a lossless conversion from pairwise alignments between sequences to a variation graph encoding the sequences and their alignments.
As input we typically take all-versus-all alignments, but the exact structure of the alignment set may be defined in an application specific way.
This algorithm uses a series of disk-backed sorts and passes over the alignment and sequence inputs to allow the graph to be constructed from very large inputs that are commonly encountered when working with large numbers of noisy input sequences. 
Memory usage during construction and traversal is limited by the use of sorted disk-backed arrays and succinct rank/select dictionaries to record a queryable version of the graph.

%## squish graph induction algorithm

As input we have $Q$, which is a concatenation of the sequences from which we will build the graph.
We build an compressed suffix array (CSA) mapping sequence names to offsets in $Q$, and also the inverse using a rank/select dictionary on a bitvector marking the starts of sequences in $Q$.
This allows us to map between positions in the sequences of $Q$, which is the format in which alignment algorithms typically express alignments, and positions in $Q$ itself, which is the coordinate space we will use as a basis for the generation of our graph.
To relate the sequences in $Q$ to each other we apply a function $map$ to generate alignments $A$.
Although these alignments tend to be represented using oriented interval pairs in $Q$, for simplicity and robustness to graph complexity, we describe $A$ as a vector of pairs of bidirectional positions (sequence offsets and strands) $b$ in $Q$ , such that $A = (b_{q}, b_{r}), \ldots $.
We sort $A$ by the first member ($b_{q}$) of each pair, ensuring that the entries in $A$ are ordered according to their order in $Q$.

To query the induced graph we build a rank/select dictionary allowing efficient traversal of $A$, based on a bit vector $A_{bv}$ of the same length as $A$ such that we record a 1 at those positions which correspond to the first instance of a given $b_{q}$ and record a 0 in $A_{bv}$ otherwise. 
We record which $b_{q}$ we have processed in the bitvector $Q_{seen}$ which is of the same length as $Q$.
This allows us to avoid a quadratic penalty in the order of the size of the transitive closures in $Q$ given by the $map$ function.

Now we inductively derive the graph implied by the alignments.
For each base $b_{q}$ in $Q$, we find its transitive closure $c_{q}$ := [$b_{q}$, $b_{r_{1}}$, ... ] via the $map$ operation by traversing the aligned base pairs recorded in $A$.
We write the character of the base $b_{q}$ to a vector $S$, then for each $b_{c}$ in $c_{q}$ we record a pair [$s_{i}, b_{c}$] into $N$ and its reverse, [$b_{c}$, $s_{i}$] into $P$.
We mark $Q_{seen}$ for each base in each emitted cluster, and we do not consider marked bases in subsequent transitive closures.
By sorting $N$ and $P$ by their first entries, we can build rank/select dictionaries on them akin to that we built on $A$ that allow random access by graph base (as given in $S$) or input base (as given in $Q$).

To fully induce the variation graph we need to establish the links between bases in $S$ that would be required for us to find any sequence in the input as a walk through the graph.
We do so by rewriting $Q$ (in both the forward and reverse orientation) in terms of pairs of bases in $S$, then sorting the resulting pairs by their first element, which yields $L$ = [($b_{a}$, $b_{b}$), ... ].
These pairs record the links and their frequencies, which we can emit or filter (such as by frequency) as needed given particular applications.
In typical use we take the graph to be given by the unique elements of $L$.

Our data model encodes the graph using single-base nodes, but often downstream use requires identifying nodes and thus we benefit from compressing the unitigs of the graph into single nodes, which reduces memory used by identifiers in analysis.
We can compress the node space of the graph by traversing $S$, and for each base querying the inbound links.
Maintaining a bitvector $S_{id}$ of length equal to $S$ we mark each base at which we see any link other than one from or to the previous base on the forward or reverse strand, or at bases where we have no incoming links.
By building a rank/select dictionary on $S_{id}$ we can assign a smaller set of node ids to the sequence space of the graph.

Given the id space encoded by $S_{id}$ we can materialize the graph in a variety of interchange formats, or provide id-based interfaces to the indexed squish graph.
To generate graphs in {\tt vg}  or GFA format, we want to decompose the graph into its nodes ($S$), edges ($L$) and paths ($P$).
The nodes are given by $S$ and $S_{id}$, and similarly we project $L$ and $P$ through $S_{id}$ to obtain a compressed variation graph.


\section{Data interchange}
%*0.5p 0.5h*

{\tt vg}'s design largely preceeded its development.
The mathematical concepts described in the previous section lie at the core of its implementation.
A schema language, Google Protocol Buffers (Protobuf), is used to define a compact description of data structures sufficient for the representation of all the required data models.
One cause of this pattern was my involvement in the GA4GH-DWG at the beginning of my thesis.
As the GA4GH is chartered to encourage focus on data interchange, the GA4GH-DWG was then seeking a coherent way of describing graph genomes\footnote{\url{https://github.com/ga4gh/ga4gh-schemas}} to allow support pangenomic principles resequencing.
I thus implemented the schema for {\tt vg} in the popular Protobuf schema language.
This provided a core API on which to bulid {\tt vg}.
It also implied a set of streaming data formats, which I implemented as a template library capable of serializing any stream of Protobuf objects.
Due to the reliance on Protobuf, the code requried to implement reading and writing of these formats is only the {\tt vg} schema and the stream library\footnote{At the time of writing the schema, \url{https://github.com/vgteam/vg/blob/master/src/vg.proto} and stream parsing library \url{https://github.com/vgteam/vg/blob/master/src/stream.hpp} total around 1000 LOC, and are sufficient to link any C++ program into the {\tt vg} ecosystem.}.
This greatly simplified the process of developing libraries for working with the variation graph data models.
Although in practice the Protobuf data structures are slower to parse than handmade C-struct serializations like BAM, the amount of effort required to begin writing efficient and structured binary data formats was considerably less with the schema based approach.
Most importantly, the schema based definition of the core data types in {\tt vg} helped new developers and researchers using the system quickly appreciate the basic concepts.
It is not easy to measure, but I believe this has been a key cause of the success of the project and unifying many different research interests spread across many groups.

%The graphical basis of {\tt vg}'s data model made it simple to adapt to RDF.

\section{Index structures}
%*0.5p 0.5h*

\subsection{XG index}
%*2p 3h*

\subsection{GCSA2 index}
%*1p 2h*

\subsection{Haplotype indexes}
%*1.5p 2h*

\subsection{Generic disk backed indexes}
%*0.5p 0.5h*

\subsection{Coverage index}
%*1p 1h*


\section{Sequence alignment to the graph}
%*0.5p 0.5h*

\subsection{POA and GSSW}
%*2p 2h*

\subsection{Banded global alignment}
%*0.5p 0.5h*

\subsection{X-drop DP}
%*0.5p 0.5h*

\subsection{MEM finding}
%*2p 2h*


\subsection{Collinear chaining}
%*0.5p 0.5h*

\subsection{Distance estimation}
%*1p 1h*

\subsection{MEM chaining}
%*2.5p 3h*

\subsection{Chunked alignment}
%*1p 1h*

\subsection{Alignment surjection}
%*1p 1h*


\subsection{Finding the alignment}
%*0.5p 0.5h*

\subsection{Mapping quality}
%*2p 2h*


\section{Visualization}
%*0.5p 0.5h*

\subsection{Hierarchical layout}
%*0.5p 0.5h*

\subsection{Force directed models}
%*1p 1.5h*

\subsection{Linear time visualization}
%*1p 2h*


\section{Graph mutating algorithms}
%*0.5p 0.5h*

\subsection{Edit}
%*1p 1h*

\subsection{Pruning}
%*2.5p 3h*
\subsubsection{$k$-mer complexity reduction}
\subsubsection{High degree filter}

\subsection{Graph sorting}
%*1p 1h*

\subsection{Graph simplification}
%*0.5p 0.5h*


\section{Graphs as basis spaces for sequence data}
%*2p 2h*

\subsection{Coverage maps}
%*1p 1h*

\subsection{Variant calling}
%*3p 2h*

\subsection{Bubble decompositions and likelihood spaces}
%*2p 4h*

\subsection{MEM matching to the bidirectional GBWT}
%*1p 3h*


\section{Contributions to related computational methods}

% in this chapter I've described things that I've done
% as a consequence of being in this work, here are things that I helped other people to build
% list papers, give references

% gpBWT
% GCSA2
% ...
